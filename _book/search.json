[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Looks at Discrimination",
    "section": "",
    "text": "Overview\nDiscrimination is a key social issue in the US and in a number of other countries. There is lots of available data with which one might investigate possible discrimination. But how might such investigations be conducted?\nOur dsld package provides both graphical and analytical tools for this purpose. It is widely applicable; here are just a few use cases:\nThis book provides a tutorial regarding applicable methodology, as well as introduction to use of the package."
  },
  {
    "objectID": "index.html#prerequisite-background",
    "href": "index.html#prerequisite-background",
    "title": "Data Science Looks at Discrimination",
    "section": "Prerequisite background",
    "text": "Prerequisite background\nIn addition to having rudimentary skill in R, the user should have a very basic knowledge of statistical inference–mean, variance, confidence intervals and tests, and histograms. A short, intuitive, “bare bones” refresher, focused on confidence intervals, is given in Appendix A.\nMathematical background, e.g. linear algebra, calculus and probability theory, are not required, and concepts are explained via intuitive language."
  },
  {
    "objectID": "index.html#the-dsld-package",
    "href": "index.html#the-dsld-package",
    "title": "Data Science Looks at Discrimination",
    "section": "The dsld package",
    "text": "The dsld package\nThe R package dsld, which this tutorial uses for examples, has two  aims:Python wrappers are included for many functions.\n\nTo enable exploratory analysis of possible discrimination effects through various graphical and tabular functions.\nTo enable formal statistical analysis of such effects via both enhanced access to general R functions such as lm() and glm() and various functions from the qeML machine learning package."
  },
  {
    "objectID": "intro.html#sec-ucb",
    "href": "intro.html#sec-ucb",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.1 UC Berkeley discrimination claims",
    "text": "1.1 UC Berkeley discrimination claims\n\nUC Berkeley was accused of discriminating against female applicants for graduate school, and indeed the overall acceptance rate for women was lower than that for men. This seemed odd, given Berkeley’s liberal reputation.\nHowever, upon breaking the data down  according to the program students were applying to, it was found that in every department, the female acceptance rate within that department was either higher than the male rate or of similar level. The problem: women were applying to more selective programs, causing their overall rate to below that of men.This data is included in R, as the built-in dataset UCBAdmissions."
  },
  {
    "objectID": "intro.html#sec-census",
    "href": "intro.html#sec-census",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.2 US Census data",
    "text": "1.2 US Census data\n\nThe svcensus dataset is a subset of US census data from back in 2000, focusing on six engineering occupations.  The question at hand is whether there is a gender pay gap. Again, the overall pay for men is higher, by about 25%. But what if we break things down by occupation? Though it does turn out that some occupations pay more than others, and that men and women are not distributed evenly among the occupations, there still is a gender pay gap, of about 16%.Included here in the dsld package."
  },
  {
    "objectID": "intro.html#commonality",
    "href": "intro.html#commonality",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.3 Commonality",
    "text": "1.3 Commonality\nIn both examples, we have an outcome variable Y of interest\\(\\textemdash\\)acceptance  rate and wage income\\(\\textemdash\\)and a sensitive variable S, which was gender in both examples. But in both cases, were are concerned that merely comparing mean Y for each gender was an oversimplication, due to a possible confounder C\\(\\textemdash\\)department in the first example, occupation in the second\\(\\textemdash\\) that is related to both variables. Failure to take confounders (there can be more than one, and usually are so) into account can lead to spurious “relations” between S and Y.As noted earlier, this book avoids technical definitions, keeping to the intuitive, including our notion here of a confounder. Actually, a fully precise definition is rather problematic.\n\n\n\n\n\n\nConfounder adjustment analysis\n\n\n\nSo, in general, we wish to estimate the impact of a sensitive variable S on an outcome variable Y, but accounting for confounders C. Let’s call such analysis “confounder adjustment.”\n\n\nThe above discussion summarizes the goal of Part I of this book. Now contrast the above examples with a different kind, which will concern Part II:"
  },
  {
    "objectID": "intro.html#compas-recidivism-data",
    "href": "intro.html#compas-recidivism-data",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.4 COMPAS recidivism data",
    "text": "1.4 COMPAS recidivism data\nCOMPAS is a commercial machine learning software tool for aiding judges in sentencing defendants convicted of a crime. The tool’s main function is to predict recidivism by a defendant. But a 2016 Pro Publica investigation found that the tool to be racially biased; African-American defendants tended to be given harsher ratings\\(\\textemdash\\)i.e. higher estimated probabilities of recidivism\\(\\textemdash\\)than similarly situated white defendants.\nNorthpointe, the firm that developed COMPAS, rejects the Pro Publica analysis, and we are not supporting either side here.\n But if the COMPAS tool were in fact biased, how could the analysis be fixed?Note that both Pro Publica’s analysis and that of Northpointe used methodology like that presented in this book.\nA key point is that any remedy must not only avoid using race directly, but must also minimize the impact of variables O that are separate from race but still correlated with it, known as proxies. If, say, educational attainment is correlated with race, the inclusion of educational in our analysis will mean that race is still playing a role in our analysis after all.\n\n\n\n\n\n\nFair ML analysis\n\n\n\nThus our goal is to predict the outcome variable Y, without using the sensitive variable S, while making only limited use of the proxy variables O."
  },
  {
    "objectID": "intro.html#summary-the-two-kinds-of-discrimination-analysis-covered-here",
    "href": "intro.html#summary-the-two-kinds-of-discrimination-analysis-covered-here",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.5 Summary: the two kinds of discrimination analysis covered here",
    "text": "1.5 Summary: the two kinds of discrimination analysis covered here\nNote the difference between accounting for confounders on the one hand, and fair ML on the other. Here is a side-by-side comparison:\n\n\n\naspect\nconfounder adjustment\nfair ML\n\n\n\n\ngoal\nestimate an effect\npredict an outcome\n\n\nharm\ncomes from society\ncomes from an algorithm\n\n\nside info\nadjust for confounders\nlimit impact of proxies\n\n\n\nPart I, on confounder adjustment, focuses on discrimination examples but is applicable to confounder adjustment applications in general. Part II, on fair ML, is more specific to discrimination settings."
  },
  {
    "objectID": "intro.html#summary-of-symbols",
    "href": "intro.html#summary-of-symbols",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.6 Summary of symbols",
    "text": "1.6 Summary of symbols\nThe symbol X will at first denote all the the variables other than Y and S. The general terminology is that Y is variously called the outcome variable, target variable or dependent variable; the X variables are known collectively as covariates, features or independent variables.\nAmong the variables in X, we will separate out some to play the role of C (Part I) or O (Part II), after which X will refer to all variables other than Y, S, C and O.\nIn terms of the above examples, here are the roles of the variables:\n\n\n\nexample\nY\nC\nS\nO\n\n\n\n\nUCB admits\nacceptance\ndepartment\ngender\n-\n\n\nCensus\nwage\ne.g. occupation\ngender\n-\n\n\nCOMPAS\nrecidivate\n-\nrace\ne.g. education"
  },
  {
    "objectID": "intro.html#a-word-before-getting-started",
    "href": "intro.html#a-word-before-getting-started",
    "title": "1  Introduction and Motivating Examples",
    "section": "1.7 A word before getting started",
    "text": "1.7 A word before getting started\nAs noted, the book consists of two main topics:\n\nPart I, adjusting for confounders\nPart II, fair ML\n\nIn each case, we present explanations of the relevant concepts, so that this is a general tutorial on methodology for analysis of discrimination, and show the details of using our dsld package to make use of that methodology.\nSo, let’s get started. One key point first, though:\n\n\n\n\n\n\nNotes on modeling, the role of the software, etc.\n\n\n\nThis book makes use of the dsld software, but is definitely not a user manual for that package. Instead, it is a guide to the statistical principles, with the software playing a supporting role.\nAny statistical model is approximate.  And virtually any relation of interest in practice is nonzero. Modern statistical thinking places reduced emphasis on significance tests and p-values, and asks instead whether A has an effect on B that is substantial enough to be of interest.\nA related point is that, unlike some readers may have experienced in some statistics courses, real-world statistical analysis is not conducted in a formulaic, “Step 1, Step 2,…” manner. Instead, decisions on say, which model to use, must be made by you, the analyst, based on your overall assessment of the available information. The software cannot make your decisions for you.\n\n\n\n\nThe pioneering statistician George Box famously said, “All models are wrong, but some are useful.”"
  },
  {
    "objectID": "PartI.html#linear-model-example-a-simple-gender-wage-gap-analysis",
    "href": "PartI.html#linear-model-example-a-simple-gender-wage-gap-analysis",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.1 Linear model example: a simple gender wage gap analysis",
    "text": "2.1 Linear model example: a simple gender wage gap analysis\nConsider the svcensus data example in Section 1.2 above, investigating a possible gender pay gap. So Y is wage and S is gender. We might treat age as a confounder C, reasoning as follows. Older workers tend to have more experience and thus higher wages, and if there is an age differential in our data, say with female workers tending to be older, this may mask a gender pay gap: If men make more money than women of the same age, but women tend to be old, the gender and age effects may largely cancel out.\nSo, let’s take the set of confounders C to consist of age, and for simplicity in this introductory example, not include any other confounders, such as occupation, and not include any other variables X.\n\n2.1.1 Initial analysis\nOur linear model would thus be\n\nmean W = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 M\n\nwhere W is wage, A is age and M is an indicator variable, with M = 1 for men and M = 0 for women. The parameters \\(\\beta\\)i are estimated by fitting the model to the data: The column svcensus$gender is an R factor. Our function dsldLinear calls R’s lm, which replaces that column by a dummy variable gendermale, our M above. If a factor has f levels, i.e. represents f categories, R will create f-1 dummies.\n\nsvcensus1 &lt;-\n   svcensus[,c(1,4,6)]  # age, wage, gender\nz &lt;- dsldLinear(svcensus1,'wageinc','gender')\ncoef(z)  # print the estimated coefficients b_i \n\n$gender\n(Intercept)         age  gendermale \n 31079.9174    489.5728  13098.2091 \n\n\nLet’s use bi to denote our estimated \\(\\beta\\)i. So for instance b1 = 489.5728 is our estimate of the unknown population parameter \\(\\beta_1\\).\nThe bi are computed using least squares, which find the bi that minimize the sum of the square of the differences between the observed Y and fitted Y.\n\n\n2.1.2 Interpretation of \\(\\beta\\)2\nLots in the output to discuss, which we will gradually cover below. For now, note that the estimate b2 turns out to be about $13,000, which is the (estimated) wage gap, if any. Here’s why: Always keep in mind that statistical quantities are only estimated, since we work only with sample data from some population, real or conceptual. Hence the need for standard errors, confidence intervals and so on.\nUnder the model, the mean wage for, say, 36-year-old men is\n\n\\(\\beta\\)0 + 36 \\(\\beta\\)1 + 1 \\(\\beta\\)2\n\nwhile for women of that age it is\n\n\\(\\beta\\)0 + 36 \\(\\beta\\)1\n\nThe difference is \\(\\beta\\)2. But if we look at, for instance, people of age 43, the mean wages for men and women are\n\n\\(\\beta\\)0 + 43 \\(\\beta\\)1 + 1 \\(\\beta\\)2\n\nand\n\n\\(\\beta\\)0 + 43 \\(\\beta\\)1\n\nand the difference is still \\(\\beta\\)2.\n\n\n\n\n\n\n“The” effect of gender\n\n\n\nThus we can speak of \\(\\beta\\)2 as the gender wage gap, at any age. According to the model, younger men earn an estimated $13,000 more than younger women, with the same-sized gap between older men and older women.\n\n\nThe above approach to dealing with confounders is a very common one. But it raises questions, such as:\n\nWhat are the assumptions underlying that model? And how might we  check whether they are (approximately) valid?\nWe chose only one C variable here, age. We might also include occupation, as noted earlier. In some datasets, might have dozens of possible confounders. How do we choose which ones to use in our model? And for that matter, why not use them all?\nThe above model, in which the gender wage gap was uniform across all wages, may not be adequate. How can we determine this, and what alternative models might we use?\n\nIn addition, the data here are, as is commonly the case, observational, as opposed to being the result of a randomized clinical trial; there may be serious issues, due to unobserved confounders. Such problems might be solvable via an advanced (and rather controversial) methodology known as causal inference. Unfortunately, details are beyond our scope in this tutorial, but we will explain some basic concepts in Section 5.2.\n\n2.1.3 Statistical inference\nThe full output of dsldLinear() goes to the heart of discrimination analysis, enabling statistical inferences on differences in levels of the sensitive variable S. Let’s take a look, continuing from the above code:\n\nsummary(z)\n\n$`Summary Coefficients`\n    Covariate   Estimate StandardError PValue\n1 (Intercept) 31079.9174    1378.08158      0\n2         age   489.5728      30.26461      0\n3  gendermale 13098.2091     790.44515      0\n\n$`Sensitive Factor Level Comparisons`\n         Factors Compared Estimates Standard Errors P-Value\nEstimate    male - female  13098.21        790.4451       0\n\n\nThe first half of this output is from lm(), which is called by dsldLinear(). The second half is the “value added” material from dsld.\nSo, an approximate 95% confidence interval for the gender wage gap is Since the estimated gender gap here is simply b2, the CI could of course have also been obtained directly from the lm half of the output. But with an S having more than two levels, e.g. race, the dsld enhancement is quite valuable.\n\n13098.2091 ± 1.96 x 790.4451\n\nor (11548.94,14647.48).\n\n\n2.1.4 With-interactions model\nAs discussed above, in our model\n\nmean W = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 M\n\nwe identified b2 as the difference in mean wage between men and women, regardless of age, so that for instance:\n\nAccording to the model, younger men earn about $13,000 more than younger women, with the same-sized gap between older men and older women.\n\nBut that may not be true. On the contrary, gender discrimination and  age discrimination may interact. It may be, for instance, that the gender gap is small at younger ages but much larger for older people.Interaction between two types of discrimination is called intersectionality by some analysts.\nTechnically, the with-interactions model adds a product term:\n\nmean W = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 M + \\(\\beta\\)3 AM\n\nSo for example, the gender pay gap for people of age 36 is\n(\\(\\beta\\)0 + \\(\\beta\\)1 36 + \\(\\beta\\)2 1 + \\(\\beta\\)3 36) - (\\(\\beta\\)0 + \\(\\beta\\)1 36 ) =\n\\(\\beta\\)2 1 + \\(\\beta\\)3 36\nAnd at age 43, the gap is\n\\(\\beta\\)2 1 + \\(\\beta\\)3 43\nSo, this model does indeed allow for interaction between age and gender.\nHowever, this added-product-term is a bit abstract, and it is easier (and approximately equivalent) to simply fit two linear models, one for men and one for women.\n\n\n\n\n\n\nNote\n\n\n\nThe dsldLinear function includes an argument interactions. The default value is FALSE, but if TRUE, it fits separate linear models for each level of S. An additional argument ‘newData’ is now needed, through which the user specifies a data frame consisting of one or more (X,C) values at which to compare the effect of S.\n\n\n\nnewData &lt;- data.frame(age=c(36,43))\nz &lt;- dsldLinear(svcensus1,'wageinc','gender',interactions=T,\n   newData)\nsummary(z)\n\n$female\n    Covariate   Estimate StandardError PValue\n1 (Intercept) 30551.4302    2123.44361      0\n2         age   502.9624      52.07742      0\n\n$male\n    Covariate  Estimate StandardError PValue\n1 (Intercept) 44313.159    1484.82216      0\n2         age   486.161      36.02116      0\n\n$`Sensitive Factor Level Comparisons`\n  Factors Compared New Data Row Estimates Standard Errors\n1    female - male            1 -13156.88        710.9696\n2    female - male            2 -13039.27        710.7782\n\n\nIn setting that newData argument, we need one row for every variable other than Y and S. In this case, there is just one such variable, age. So, in the above call, we are fitting two linear models, one for men and another for women, then comparing regression function values at the two specified ages.\nSo the gender pay gap is estimated to be -13156.88 at age 36, and -13039.27  at age 43, differing by only about $100. The estimated gap between ages 36 and 53, not shown, is larger, close to $300, but it seems there is not much interaction here. The no-interactions model should be adequate after all.The classical approach to choosing between the no-interaction and with-interaction models is of course to test the hypothesis \\(H_0: \\beta_3=0\\). As noted earlier and detailed in Section 2.6, modern practice discourages such approaches, which can be misleading.\n\n\n2.1.5 Linearity and other assumptions\nAs noted, linear models are ubiquitous in observational data analysis. Open any professional journal in medicine, sociology, economics and so on, and you’ll see many applications of this methodology. But how would one check that most basic assumption, the linearity of the mean Y for given X, C (or O) and S values?\n\n\n\n\n\n\nAssumptions–not just a formality\n\n\n\nAssumptions matter. They are never perfectly satisfied, but failure to be even approximately valid can mean deciding that there is no discrimination when it actually is there, or vice versa. It can mean bad medication being declared by the government as good, or vice versa. In litigation, if a key expert witness is exposed by opposing counsel as not having checked the assumptions in his/her analysis, the side for which the witness was testifying will likely lose the case on the spot.\n\n\nTypically, linearity is checked graphically. A common approach involves plotting the residuals, which are the differences between the fitted line and the Y values. Here, though, we use another graphical approach, via a dsld function that may be more informative.\nReturning to our earlier setting with just S = gender for our example, we run\nThe function plots a smoothed graph of Y against a user-specified C  variable, once for each level of S. So, the call here says, “Plot smoothed wage income against age, for each gender.”The function has a ‘conditions’ argument; we have none here, so we just used a trival one, ‘age &gt; 0’\nThe model has mean Y being a linear of function of age, so we should expect to see approximate straight lines here. Yet the relation certainly looks nonlinear, possibly reflecting age discrimination against both very young and very old workers. We are already investigating one kind of discrimination here, gender, so again for simplicity let’s just keep age as a confounder.\n\n\n2.1.6 Updated model\nBut we must do something about the substantial nonlinearity  we’ve discovered, and one possible remedy is to add an age2 term be added to the equation:Adding a squared term does not make our model nonlinear, as it is still linear in the \\(\\beta\\)i; if we, say, double each of those, the entire expression is doubled, the definition of linearity. The model is nonlinear in age but linear in the \\(\\beta\\)i.\nmean W = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 A2 + \\(\\beta\\)3 M\nLet’s fit the updated model:\n\nsvcensus1$age2 &lt;- svcensus1$age^2\nz &lt;- dsldLinear(svcensus1,'wageinc','gender')\ncoef(z)  # print the estimated coefficients b_i \n\n$gender\n  (Intercept)           age    gendermale          age2 \n-104196.65579    7251.30962   15270.56685     -79.16059 \n\n\nSo we see that the original wage gap figure of about $13,000 was incorrect; it’s actually estimated to be over $15,000, so the original model underestimated the gap by about 15%.\nWe see in this example that misspecifying a linear model can have a major impact on its accuracy. As usual, though, we will try to keep things simple, and thus use only the original linear model in our subsequent examples below.\n\n\n2.1.7 Other assumptions\nOther than linearity, the standard errors reported by lm() also  assume that variance of wage income is approximately constant across ages and genders. Lack of this property has some effect on the accuracy of reported standard errors, but this can be adjusted via the so-called sandwich operation, an option in dsldLinear().It is also assumed that wage income has a normal/gaussian distribution at each level, but the Central Limit Theorem’s implication for the sums created by lm() is that the \\(b_i\\) are in fact approximately normal. The normality assumption is not very important."
  },
  {
    "objectID": "PartI.html#s-may-consist-of-more-than-one-factor",
    "href": "PartI.html#s-may-consist-of-more-than-one-factor",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.2 S may consist of more than one factor",
    "text": "2.2 S may consist of more than one factor\nIn introducing this example, we noted the need to start simple. Let’s move away from that a bit.\nIn the svcensus data, both age and gender are potential areas of discrimination. We can treat both as such by combining these two R factor variables into one “super R factor,” as follows.\nWe’ll need to discretize age, and since federal law on age discrimination uses age 40 as the definition of “older,” let’s use that as an example:\n\nsvc &lt;- svcensus\nage &lt;- svc$age\nage &lt;- ifelse(age &gt;= 40,'40+','under40')\nage &lt;- as.factor(age)\nhead(age)\n\n[1] 40+     40+     under40 40+     40+     40+    \nLevels: 40+ under40\n\nsvc$age &lt;- age\n\nNow let’s make our “super factor,” using a function from qeML:\n\nAgeGend &lt;- cartesianFactor('svc',c('age','gender')) \nsvc$AgeGend &lt;- AgeGend\nhead(svc)\n\n      age     educ occ wageinc wkswrkd gender        AgeGend\n1     40+ zzzOther 102   75000      52 female     40+.female\n2     40+ zzzOther 101   12300      20   male       40+.male\n3 under40 zzzOther 102   15400      52 female under40.female\n4     40+ zzzOther 100       0      52   male       40+.male\n5     40+ zzzOther 100     160       1 female     40+.female\n6     40+ zzzOther 100       0       0   male       40+.male\n\n\nWe have only three education codes here, with 14 and 16 representing a Master’s degree and 16 a PhD, and ‘zzzOther’ denoting all others. Since this dataset consists of Silicon Valley programmers and engineers, the vast majority of “others” have a Bachelor’s degree.\nWe no longer need the original age and gender columns, so we’ll delete them and then try some analysis:\n\nsvc$age &lt;- NULL\nsvc$gender &lt;- NULL\nw &lt;- dsldLinear(svc,'wageinc','AgeGend')\nsummary(w)\n\n$`Summary Coefficients`\n               Covariate   Estimate StandardError       PValue\n1            (Intercept)   2482.330    1529.05560 1.044954e-01\n2                 educ16   8194.121    1717.11275 1.823745e-06\n3           educzzzOther -14554.025     747.54506 0.000000e+00\n4                 occ101   1703.896     899.52840 5.819707e-02\n5                 occ102  13627.089     829.14115 0.000000e+00\n6                 occ106   1351.235    2013.86471 5.022422e-01\n7                 occ140  11409.603    1643.94237 3.910205e-12\n8                 occ141  11407.453    1039.62019 0.000000e+00\n9                wkswrkd   1313.971      20.77242 0.000000e+00\n10       AgeGend40+.male   9834.035    1064.86666 0.000000e+00\n11 AgeGendunder40.female  -8176.053    1229.58483 2.942069e-11\n12   AgeGendunder40.male   -408.267    1030.00692 6.918298e-01\n\n$`Sensitive Factor Level Comparisons`\n               Factors Compared Estimates Standard Errors      P-Value\n1         40+.female - 40+.male -9834.035       1064.8667 0.000000e+00\n2   40+.female - under40.female  8176.053       1229.5848 2.942069e-11\n3     40+.female - under40.male   408.267       1030.0069 6.918298e-01\n4     40+.male - under40.female 18010.088        990.2130 0.000000e+00\n5       40+.male - under40.male 10242.302        705.2504 0.000000e+00\n6 under40.female - under40.male -7767.786        951.1388 2.220446e-16\n\n\nAh, this is a more nuanced probe than the ones above in which we simply  used age as a confounder. The male-female differences at both the older and younger age levels, about $9800 and $7800, are both substantial, but smaller than the $13,100 overall figure we obtained earlier.This is an example of Simpson’s Paradox, in which an overall average effect might be larger than the terms that make up the average. In fact, the algebraic signs may change, as we saw with the UC Berkeley admissionss data in Section 1.1.\nNote too the impact of age within genders. Older women made about $8200 more than younger women, but for men the figure was rather larger, about $10,200.\nOn the other hand, this analysis is probably too coarse with respect to age, as it does not reveal the negative impact of age well beyond 40. It may be worth trying a finer discretization of age, say, 35-, 35-55 and 55+."
  },
  {
    "objectID": "PartI.html#the-logistic-model",
    "href": "PartI.html#the-logistic-model",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.3 The Logistic model",
    "text": "2.3 The Logistic model\nThe logistic model is an example of a generalized linear model, whose name stems from it having a linear component in the formula, as will be seen below.\n\n2.3.1 General form of the model\nJust as linear models are the most commonly used for numeric Y, in the binary-Y case the go-to standard is the logistic model. To introduce it, let’s first consider a very simple prediction problem, in which Y is gender, say 1 for male, 0 for female, and X is simply income, using the svcensus data (no C here).\n\n\n\n\n\n\nProbability is a special case of a mean\n\n\n\nNote that mean Y is now the probability that Y = 1. That’s because the average of a bunch of 1s and 0s is the proportion of 1s. In the data 1,0,1,1 for instance, the mean is (1+0+1+1) / 4 = 3/4, and indeed 3/4 of those numbers are 1s.\n\n\nSuppose that within each gender, X has a normal (Gaussian) distribution, the familiar “bell-shaped” curve, with the same standard deviation for each gender. Then it turns out that one can show mathematically that\nprobability male = \\[\n\\frac\n{1}{1 + e^{-(\\beta_0 + \\beta_1 income)}}\n\\]\nThe values \\(\\beta_0\\) and \\(\\beta_1\\) actually are expressions involving quantities such as the per-gender mean incomes, but the point is that in the end the probability has a linear component to it, \\(\\beta_0 + \\beta_1 income\\).\nThat formula follows the form of the logistic function, \\(f(t) = 1 / [1 + e^{-t}]\\), which has the shape of an S-curve:\n\ncurve(1/(1+exp(-x)),-4,4)\n\n\n\n\nSo at least the model retains a linear component, with much the same interpretability. For instance, if \\(\\beta_2 &gt; 0\\), then the usual monotonicity relation holds, i.e. the higher the income, the greater the probability that the person is male.\nThe estimates bi of the population values \\(\\beta_i\\) are obtained via a method generalizing the least-squares method used in the linear case.\n In the case of multiple predictor variables, the logistic form can again be motivated by considering within-group distributions:There is a similar situation for the linear case. If Y and the predictor variables have a multivariate normal distribution, one can show that mean Y as a function of the features is linear in the features, etc.\nSay we predict gender from age and wage income. If the latter two variables have a bivariate normal distribution (two-dimensional histogram has a 3-D bell shape) with the same variance matrices within each gender, it turns out that we again get a logistic form:\nprobability male = \\[\n\\frac\n{1}{1 + e^{-(\\beta_0 + \\beta_1 age + \\beta_2 income)}}\n\\]\nNow, what about that assumption of the normal distributions and so on? Just as many regression functions for numeric Y in practice are roughly linear, in predicting binary Y the S-curve model is often roughly valid. Moreover, the logistic model has two desirable properties for predicting binary Y:\n\nIts value is between 0 and 1, appropriate for modeling a probability.\nAs noted earlier, the expression \\(1 / [1 + e^{-t}]\\) is increasing in t, which we wish to model when our predictors have monotonic relations with Y.\n\nThe point is that the logistic (popularly referred to as “logit”) is often a good model for binary-Y settings in general.\n\n\n2.3.2 We no longer have a no-interactions case\nIn our earlier linear model, predicting wage income from age and gender,\n\nmean W = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 M\n\nrecall that here \\(\\beta\\)2 has the nice interpretation of there being a uniform gender gap, independent of age. Similar statements hold for \\(\\beta_1\\); a 1-year increase in age, for instance, on average is associated with a \\(\\beta_1\\) increase in mean income, identically for either gender.\nGeometrically, if we were to plot separate male and female regression lines against age, the male and female r lines would be parallel. That’s not possible in the logit case, as logit curves cannot be parallel:\n\ncurve(1/(1+exp(-x)),-4,4)\ncurve(1/(1+exp(-2*x)),-4,4,add=TRUE)\n\n\n\n\nThe curves are no longer parallel; they even cross. They typically don’t cross in applied settings, but they are always nonparallel.\nThe implication of this is the same as in the linear case. We cannot speak of “the” impact of S on Y, as it will not be the same at different levels of the X and C variables. So there is no direct analog of the no-interactions case for linear models, in which we could speak of \\(\\beta_2\\) as being “the” gender pay gap.\nSome books motivate the logistic approach as the log-odds ratio, meaning in this example that the logarithm of the ratio (probability male) / (probability female) is linear in age and income.\n\nlog [probability male / probability female] = \\(\\beta\\)0 + \\(\\beta\\)1 A + \\(\\beta\\)2 W\n\nSo here we do have a formulation in which the impact of wage is the same  for any age level, albeit on this much less-interpretable log scale.Note that a log-odds measure can take on any value betwen \\(-\\infty\\) and \\(\\infty\\).\n\n\n2.3.3 Example: mortgage data\nThis dataset and its documentation are included in the SortedEffects  package. The issue here is whether racism played a role in mortgage denials in the Boston area. As this is a binary outcome, we might consider a logit model.We have done small modifications, to create R factors for some columns.\nLet’s try a no-interactions model:\n\ndata(mortgageSE)\nz &lt;- dsldLogit(mortgageSE,'deny','black',yesYVal='1') \nsummary(z)\n\n$`Summary Coefficients`\n     Covariate    Estimate Standard.Error       PValue\n1  (Intercept) -4.68021873     0.73384720 1.798289e-10\n2       p_irat  5.15585388     1.06258659 1.221161e-06\n3       black1  0.64088712     0.18356747 4.806957e-04\n4      hse_inc -0.83228276     1.28336021 5.166497e-01\n5     loan_val  0.20205974     0.70171872 7.733852e-01\n6       ccred2  0.72353389     0.21417086 7.293486e-04\n7       ccred3  0.89094547     0.31542102 4.733628e-03\n8       ccred4  1.56403348     0.33660382 3.375955e-06\n9       ccred5  1.18009433     0.24808769 1.967220e-06\n10      ccred6  1.55681669     0.23373704 2.728094e-11\n11      mcred2  0.32167763     0.19927066 1.064678e-01\n12      mcred3  0.45713207     0.47817345 3.390741e-01\n13      mcred4  0.21616464     0.66923366 7.466928e-01\n14     pubrec1  1.26756099     0.21044970 1.711007e-09\n15     denpmi1  4.61566552     0.56425832 2.837083e-16\n16    selfemp1  0.63445399     0.21665184 3.406571e-03\n17     single1  0.41614600     0.15962315 9.132523e-03\n18     hischl1 -1.11171191     0.42286015 8.562887e-03\n19    probunmp  0.05565252     0.03459848 1.077202e-01\n20      condo1 -0.12673301     0.17493093 4.687744e-01\n21    ltv_med1  0.42773600     0.21618162 4.786155e-02\n22   ltv_high1  1.50099888     0.42183272 3.732914e-04\n\n$`Sensitive Factor Level Comparisons`\n         Factors Compared Estimates Standard Errors      P-Value\nEstimate            1 - 0 0.6408871       0.1835675 0.0004806957\n\n\nSo, being Black resulted in an average increase in log-odds of about 0.6409. A 95% confidence interval is 0.6409 \\(\\pm\\) 1.96 x 0.1836 = (0.2810,1.0008). By comparison, being self-employed, for instance, has a similar estimated coefficient of about 0.6355."
  },
  {
    "objectID": "PartI.html#machine-learning-approaches",
    "href": "PartI.html#machine-learning-approaches",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.4 Machine learning approaches",
    "text": "2.4 Machine learning approaches\nWe referred to the quantities \\(\\beta_i\\) above as “parameters.” The linear and logistic models are thus called parametric models. In each case, the regression function is modeled as linear, “S curve-shaped” and so on, that can be described with just a few parameters.\nBut these models make assumptions, such as assuming the regression function is approximately linear. It would be nice to have available methods that don’t rely on such assumptions.\nMachine learning (ML) algorithms typically do not assume the regression function has any particular form. They are thus “safer,” though on the other hand they are less interpretable than, say, that \\(\\beta_2\\) quantity in our no-interactions linear model above. Nor are standard errors available for regression function values at given points.\nML is mainly concerned with prediction, while we here are interested in estimation of effect sizes. However, ML algorithms either directly or indirectly do their prediction by estimating the regression function, as we do here, so they can be quite useful in our context.\n\n2.4.1 The k-Nearest Neighbor algorithm\nWe’ll focus here on the k-Nearest Neighbor (k-NN) algorithm, as it is the simplest to explain. Say we are predicting people’s weights, knowing only their heights. We have training data, on which we know both the height and weight of each person. Faced with a new case of a person 70.2 inches tall but with unknown weight, we find the people in our training set whose heights are close to 70.2 inches, and average their weights. That average is our predicted weight for the new person, as well as our estimate for the value of the regression function at height = 70.2.\nHow is “close to” defined? Should we look at the closest 5 people, the closest 50, or what? That number is k, the number of nearest neighbors.  It’s chosen by the analyst, just like the analyst chooses the number of bins in plotting a histogram. Typically k is chosen by trying several different values of k, then using the one that best predicts in the holdout set. Once we’ve settled on k, we might use the full dataset, no holdout.The number k here is called hyperparameter of the algorithms.\n\nset.seed(9999)\nz &lt;- dsldML('svcensus',quote(wageinc),'gender',\n   qeMLftnName='qeKNN',opts=list(k=50))  \nprint(z)  \n\n$testAccs\n$testAccs$female\nNULL\n\n$testAccs$male\nNULL\n\n\n$comparisons\n           age     educ occ wkswrkd female  male\n10502 37.43326 zzzOther 101      52  48914 61156\n19689 47.75017 zzzOther 100      52  57628 60422\n13609 42.46283 zzzOther 102      52  66182 86474\n12419 46.67517       14 100      52  69834 85650\n14402 52.66371 zzzOther 106      52  59200 69752\n\n\nThe first three arguments are as usual. The fourth states that the ML  algorithm we wish to use is kNN; the qeML package also includes random forests, boosting and neural networks. The opts argument states which non-default values we wish to use for arguments to the ML function; the default k actually is 25.The use of R’s quote function here is due to a nuance in the way R handles nested quotes.\nThe comparison cases are by default a random sample of 5 rows of the training set. In order to obtain consistent results each time this book is processed by Quarto, we’ve called R’s set.seed function.\nIt’s important to keep in mind that the output here is not simply for 5 persons; each row represents a subpopulation. Thus for instance in that first row of output, we estimate among all Silicon Valley techies of age 37.4, with a Bachelor’s degree or less, in occupation 101, and having worked 52 weeks, women’s average wage is 48,914 and for men it’s 61,156.\nAs before, we see a substantial gender wage gap, but now we can feel more confident about it, since this analysis is unencumbered by assumptions on the form of the regression function. On the other hand, as noted, there is no easy way to obtain standard errors.\n\n\n2.4.2 The random forests algorithm\nIn the above example, in which we are predicting wage income, a random forests (RF) analysis would generate a large number of decision trees. As explained below, each tree would give us a predicted income, and our final prediction would simply the average of all those predicted values.\nSo, what is a decision tree? It’s just a flow chart. Consider this example involving vertebral disease. Y is categorical, with values NO (normal), SL and DH. To predict a new case, we look at the X variables (which have the nondescript names V1 through V6) one at a time, and branch through the tree accordingly.\nFor example, if the new case to be predicted has V6 of at least 16, we branch right, and immediately decide to predict that this patient’s disease status is SL. If on the other hand, V6 is less than 16, we look at V4. If it’s under 28, we immediately predict status DH, and so on.\n\n\n\nA decision tree\n\n\nThe order in which variables are considered in building a tree is random for each tree. So, while V6 is considered first in the tree displayed here, in some other tree it may be, say, V2. The split points, e.g. 16 in the root node in the picture, are obtained through formulas that depend on the data in complex (though not terribly deep) ways.\nThere is no “universally best” predictive algorithm, with performance of a given algorithm being dependent on the given dataset. So, instead of doing a kNN analysis, we could try RF:\n\nset.seed(9999)\nw &lt;- dsldML('svcensus',quote(wageinc),'gender',\n   qeMLftnName='qeRF')  \n\nLoading required namespace: randomForest\n\nprint(w)  \n\n$testAccs\n$testAccs$female\nNULL\n\n$testAccs$male\nNULL\n\n\n$comparisons\n           age     educ occ wkswrkd   female     male\n10502 37.43326 zzzOther 101      52 51674.50 71698.44\n19689 47.75017 zzzOther 100      52 47598.81 67647.72\n13609 42.46283 zzzOther 102      52 72271.37 83922.60\n12419 46.67517       14 100      52 59621.01 63616.03\n14402 52.66371 zzzOther 106      52 56677.27 62521.89\n\n\nThough still indicating a gender wage gap, the results here are rather different from what we obtained with kNN. Which one is better in this particular application? Again, we may decide this on the basis of prediction accuracy (testAcc values) on the holdout set. The RF version looks slightly better, but for full thoroughness, we would need to try different hyperparameter values, and so on. Short of going through all that, the two models are similar.\n\n\n2.4.3 Other ML methods\nThe most famous class of ML these days is of course neural networks, especially the refined version deep learning. These have been found highly useful in image recognition and language processing.\nIn contrast to these fields of application, the ML people refer to the types of data we see in this book, where each row represents, say, one person, as tabular data. Their go-to method for such data is gradient boosting (GB), more specifically XGBoost, accessible in qeML via qeXGBoost. GB is again a tree-based method, but it works by starting with a primitive tree and repeatedly refining it."
  },
  {
    "objectID": "PartI.html#sec-law-school",
    "href": "PartI.html#sec-law-school",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.5 Example: The Law School Admissions dataset",
    "text": "2.5 Example: The Law School Admissions dataset\nThis is the dataset law.school.admissions, included in dsld via  fairml. It’s quite well-known in the ML world, but its full provenance is unclear. For instance, the age variable skews far to middle-aged and older, seemingly not consistent with the data’s description on Kaggle.Possible the ‘age’ variable is birth year. The data are from 1991, so an ‘age’ value of 69 would mean born in 1969, now age 22.\nThus it should be kept in mind that this is just an illustration of methodology, and firm conclusions about the legal education process should not be drawn. The data concern students who were admitted to law school, so in spite of the title, it’s not about the admissions process itself.\nThe main variables of interest here are:\n\ndecile1, decile2: Student’s standing after Year 1 and Year 3 of law school.\nfam_inc: Apparently the income level of the family in which the law students was raised in.\nlsat: Score on the LSAT, a major law school admissions test.\nugpa: Undergraduate GPA.\ngender: Gender.\nrace1: Primary racial group.\ncluster: Level of prestige of the law school.\nbar: After law school, did this person pass the bar examination?\n\nHere’s what the data looks like:\n\ndata(law.school.admissions)\nhead(law.school.admissions)\n\n   age decile1 decile3 fam_inc lsat ugpa gender race1 cluster fulltime   bar\n2   62      10      10       5 44.0  3.5 female white       1        1  TRUE\n3   62       5       4       4 29.0  3.5 female white       2        1  TRUE\n6   61       8       7       3 37.0  3.4   male white       1        1  TRUE\n7   60       8       7       4 43.0  3.3 female white       1        1  TRUE\n9   57       3       2       4 41.0  3.3 female white       4        1  TRUE\n11  59       1       1       4 24.5  2.2   male white       3        1 FALSE\n\nnames(law.school.admissions)\n\n [1] \"age\"      \"decile1\"  \"decile3\"  \"fam_inc\"  \"lsat\"     \"ugpa\"    \n [7] \"gender\"   \"race1\"    \"cluster\"  \"fulltime\" \"bar\"     \n\nlsa &lt;- law.school.admissions\n\n\n2.5.1 Is the LSAT fair?\nThere has been concern that the LSAT and other similar tests are biased against Black and Latino students, and might otherwise have racial issues. Let’s investigate, using dsld.\n\nz &lt;- dsldLinear(lsa,'lsat','race1')\nsummary(z)\n\n$`Summary Coefficients`\n     Covariate    Estimate StandardError       PValue\n1  (Intercept) 31.98578856   0.448435264 0.000000e+00\n2          age  0.02082458   0.005841758 3.641634e-04\n3      decile1  0.12754812   0.020946536 1.134602e-09\n4      decile3  0.21495015   0.020918737 0.000000e+00\n5      fam_inc  0.30085804   0.035953051 0.000000e+00\n6         ugpa -0.27817274   0.080430542 5.430993e-04\n7   gendermale  0.51377385   0.060037102 0.000000e+00\n8   race1black -4.74826307   0.198088318 0.000000e+00\n9    race1hisp -2.00145969   0.203504412 0.000000e+00\n10  race1other -0.86803104   0.262528590 9.449471e-04\n11  race1white  1.24708760   0.154627086 6.661338e-16\n12    cluster2 -5.10668358   0.119798362 0.000000e+00\n13    cluster3 -2.43613709   0.074744210 0.000000e+00\n14    cluster4  1.21094567   0.088478368 0.000000e+00\n15    cluster5  3.79427535   0.124476695 0.000000e+00\n16    cluster6 -5.53216090   0.210750853 0.000000e+00\n17   fulltime2 -1.38882076   0.116212777 0.000000e+00\n18     barTRUE  1.74973262   0.102818692 0.000000e+00\n\n$`Sensitive Factor Level Comparisons`\n   Factors Compared Estimates Standard Errors      P-Value\n1     asian - black  4.748263       0.1980883 0.000000e+00\n2      asian - hisp  2.001460       0.2035044 0.000000e+00\n3     asian - other  0.868031       0.2625286 9.449471e-04\n4     asian - white -1.247088       0.1546271 6.661338e-16\n5      black - hisp -2.746803       0.1863750 0.000000e+00\n6     black - other -3.880232       0.2515488 0.000000e+00\n7     black - white -5.995351       0.1409991 0.000000e+00\n8      hisp - other -1.133429       0.2562971 9.764506e-06\n9      hisp - white -3.248547       0.1457509 0.000000e+00\n10    other - white -2.115119       0.2194472 0.000000e+00\n\n\n There are very concerning racial differences here. Two very similar people—who attended the same quality law school, with the same undergraduate grades, the same law school grades, even having the same bar passage status—will have LSAT scores differing on average by almost 6 points if one person is Black and the other is white.Note the retrospective view–using later events to “predict” the past. This is valid, but may seem odd at first.\nAgain, one must be very cautious in drawing conclusions as to causes, not only because of the questionable quality of the dataset but also because hidden confounders may be at work here. For instance, though we have data on undergraduate GPA, we don’t know the quality of the undergraduate institution. Equally important, we are looking only at those who were admitted to law school; there may be a different pattern in the general population. But the results here raise serious concerns.\n\n\n2.5.2 Is the bar exam fair?\nAnd what about passage of the bar exam? Does race play a role?\n\ncomparisonPts &lt;- lsa[c(2,22,222,2222),-c(8,11)]\nw &lt;- dsldLogit(lsa,'bar','race1',comparisonPts,yesYVal= 'TRUE')\nsummary(w)   \n\n$`Summary Coefficients`\n     Covariate    Estimate Standard.Error        PValue\n1  (Intercept) -6.24270431    0.401137499  1.308749e-54\n2          age  0.03422736    0.004412075  8.651327e-15\n3      decile1  0.04757930    0.018427921  9.825421e-03\n4      decile3  0.48948790    0.020478831 2.909741e-126\n5      fam_inc -0.02199998    0.030373942  4.688788e-01\n6         lsat  0.08539196    0.005869773  6.036082e-48\n7         ugpa  0.35866792    0.068480789  1.627690e-07\n8   gendermale  0.15784971    0.052981691  2.888835e-03\n9   race1black  0.20964666    0.130530665  1.082497e-01\n10   race1hisp  0.07078955    0.136700672  6.045675e-01\n11  race1other  0.06268978    0.185086652  7.348320e-01\n12  race1white  0.37434379    0.111291746  7.692575e-04\n13    cluster2 -0.85200531    0.096842597  1.394793e-18\n14    cluster3 -0.15391198    0.069754152  2.734957e-02\n15    cluster4 -0.23351498    0.082896615  4.848324e-03\n16    cluster5  0.25793171    0.142650370  7.058485e-02\n17    cluster6 -1.33844995    0.145701743  4.068449e-20\n18   fulltime2 -0.54114731    0.088416220  9.330973e-10\n\n$`Sensitive Factor Level Comparisons`\n[1] \"Factors Compared\" \"Estimates\"        \"Standard Errors\"  \"P-Value\"         \n\n\nThat’s a lot of output. Let’s take a closer look.\nAt least judging from the rows labled ‘black - white’, Black and white students having the same traits appear to have passed the bar exam at about the same rates.\nWe might also do a little check of the appropriateness of the logistic model for this data. One rough approach might be to use dsldConditDisparity(), as we did in the linear case:\n\n# Y needs to be numeric, in this case 0,1\nlsab &lt;- lsa\nlsab$bar &lt;- ifelse(lsab$bar=='TRUE',1,0)\ndsldConditDisparity(lsab,'bar','race1','lsat','lsab$age &gt; 0')\n\n\n\n\nLooks pretty good. But we can go further, using k-NN analysis, as it makes no assumptions about the form of the regression function:\n\nw &lt;- dsldML('lsa',quote(bar),'race1',qeMLftnName='qeKNN',\n   opts=list(k=50,yesYVal='TRUE'))  \nprint(w)  \n\n$testAccs\n$testAccs$asian\nNULL\n\n$testAccs$black\nNULL\n\n$testAccs$hisp\nNULL\n\n$testAccs$other\nNULL\n\n$testAccs$white\nNULL\n\n\n$comparisons\n      age decile1 decile3 fam_inc lsat ugpa gender cluster fulltime asian black\n27449  57       2       1       4   29  2.7 female       1        1  0.72  0.60\n20515  60       7       8       4   32  3.4 female       3        1  1.00  0.96\n3410   61       1       1       3   23  2.7   male       3        1  0.68  0.40\n26569  51       6       7       3   34  2.7   male       3        2  0.72  0.60\n21745  61       8       7       3   42  3.3   male       2        1  0.52  0.76\n      hisp other white\n27449 0.64  0.68  0.52\n20515 0.84  1.00  0.96\n3410  0.60  0.68  0.56\n26569 0.68  0.88  0.84\n21745 0.96  0.84  0.96\n\n\nThe pattern here seems to be a bit uneven. Black and white test takers seem to be on par with each other in three cases, but with major differences in the other two rows. We should look at more cases, and revisit the logit analysis, possible checking for quadratic trends.."
  },
  {
    "objectID": "PartI.html#sec-sig",
    "href": "PartI.html#sec-sig",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.6 Case study: problems with significance testing",
    "text": "2.6 Case study: problems with significance testing\nIn 2016, the American Statistical Association released its first-ever position paper, to warn of the problems of significance testing and “p-values.” Though the issues had been well known for years\\(\\textemdash\\)some journals had even banned the use of p-values in their published research papers\\(\\textemdash\\)it was “significant” that the ASA finally took a stand. Let’s use the law school example in Section 2.5 to illustrate.\nThere is concern that the LSAT and other similar tests may be  heavily influenced by family income, thus unfair, especially to underrepresented minorities. To investigate this, let’s consider the bi, the estimated coefficients in our linear model for the LSAT above.Review: To test the hypothesis \\(H_0: \\beta_i = 0\\), one computes twice the area to the right of \\(|b_i|\\) in the standard normal distribution. That number is the significance level, with values under 0.05 being termed significant; 0.01 is the criterion for highly significant, and so on.\nIn particular, look at the coefficient for family income, 0.3009. The p-value is essentially 0, which in an academic research journal would classically be heralded with much fanfare, termed “very highly significant,” with a 3-star insignia. Indeed, the latter is seen in the output above. (This comes from R, not dlsd.) But actually, the impact of family income is not very large. Here’s why:\nFamily income in this dataset is measured by quintiles. So  this estimated coefficient says that, for example, if we compare people who grew up in the bottom 20% of income with those who were raised in the next 20%, the mean LSAT score rises by only about 1/3 of 1 point\\(\\textemdash\\)a minuscule difference on a test where scores are typically in the 20s, 30s and 40s. The 95% confidence interval (CI), (0.2304,0.3714), again indicates that the effect size here is very small.Mathematically, testing for a 0 effect is equivalent to checking whether the CI contains 0. But this is missing the point of the CI, which is to (a) give us an idea of the effect size, and (b) to indicate how accurate our estimate is of that size. Aspect (a) is given by the location of the center of the interval, while (b) is seen from the CI’s width\nSo family income is not an important factor after all, and the significance test was highly misleading.\nSome who read this may object, “Sure, there sometimes may be a difference between statistical significance and practical significance. But I just want to check whether my model fits the data.” Actually, it’s the same problem.\nFor instance, suppose we are considering adding an interaction term between race and undergraduate GPA to our above model. For simplicity, let’s use R’s linear model function, lm(), directly:\n\nw &lt;- lm(lsat ~ .,lsa)  # predict lsat from all other variables\nw1 &lt;- lm(lsat ~ .+race1:ugpa,lsa)  # add interaction \nsummary(w1)\n\n\nCall:\nlm(formula = lsat ~ . + race1:ugpa, data = lsa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1783  -2.8065   0.1219   2.8879  16.0633 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     26.574993   1.219611  21.790  &lt; 2e-16 ***\nage              0.020612   0.005837   3.531 0.000415 ***\ndecile1          0.127585   0.020926   6.097 1.10e-09 ***\ndecile3          0.213918   0.020902  10.234  &lt; 2e-16 ***\nfam_inc          0.295042   0.035939   8.210 2.35e-16 ***\nugpa             1.417659   0.363389   3.901 9.60e-05 ***\ngendermale       0.513686   0.059986   8.563  &lt; 2e-16 ***\nrace1black       4.121631   1.439354   2.864 0.004194 ** \nrace1hisp        1.378504   1.570833   0.878 0.380191    \nrace1other       2.212299   1.976702   1.119 0.263073    \nrace1white       6.838251   1.201559   5.691 1.28e-08 ***\ncluster2        -5.105703   0.119879 -42.590  &lt; 2e-16 ***\ncluster3        -2.427800   0.074862 -32.430  &lt; 2e-16 ***\ncluster4         1.208794   0.088453  13.666  &lt; 2e-16 ***\ncluster5         3.777611   0.124422  30.361  &lt; 2e-16 ***\ncluster6        -5.565130   0.210945 -26.382  &lt; 2e-16 ***\nfulltime2       -1.406151   0.116132 -12.108  &lt; 2e-16 ***\nbarTRUE          1.743800   0.102855  16.954  &lt; 2e-16 ***\nugpa:race1black -2.876555   0.460281  -6.250 4.20e-10 ***\nugpa:race1hisp  -1.022786   0.494210  -2.070 0.038508 *  \nugpa:race1other -0.941852   0.617940  -1.524 0.127479    \nugpa:race1white -1.737553   0.370283  -4.693 2.72e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 20778 degrees of freedom\nMultiple R-squared:  0.3948,    Adjusted R-squared:  0.3942 \nF-statistic: 645.4 on 21 and 20778 DF,  p-value: &lt; 2.2e-16\n\ntypx &lt;- lsa[1,-5]  # set up an example case\npredict(w,typx)  # estimated regression function value\n\n      2 \n40.2294 \n\npredict(w1,typx)  # estimated regression function value\n\n      2 \n40.2056 \n\n\nIndeed, the Black and white interaction terms are “very highly significant.” But that does mean we should use the more complex model?\nRecall that many ML algorithms, including standard linear regression, use the estimated regression function value as its predicted value. So we see here that adding in term interaction term changed the estimated value of the regession function by only about 0.02 out of a 40.23 baseline. So, we may well prefer the simpler, no-interaction model.\nAgain, we must not take small p-values literally.\n\n\n\n\n\n\nThe basic problem with significance testing\n\n\n\nThe central issue in the above examples, and essentially in any other testing situation, is that the test is not answering the question of interest to us.\nWe wish to know whether family income plays a substantial role in the LSAT, not whether they is any relation at all, no matter how meaningless. Similarly, we wish to know whether the interaction between race and GPA is substantial enough to include it in our model, not whether there is any interaction at all, no matter how tiny.\nThe question at hand in research studies is rarely, if ever, whether a quantity is exactly 0, i.e. 0.000… to infinitely many decimal places. Indeed, in most cases our measuring instrument is not this accurate in the first place, and there will always be systemic bias or missingness, unobserved variables and so on, so exact zeroness is not even a meaningful concept.\nThus in almost all cases, significance tests don’t address the issue of interest, which is whether some population quantity is substantial enough to be considered important. Analysts should not be misled by words like “significant.” As noted, modern statistical practice places reduced value, or in the view of many, no value at all, on significance testing.\n\n\nTo be sure, not all statistical professionals are “modern.” Some on the ASA committee that produced the position paper were more reserved on the matter, resulting in the paper not going further than some would have preferred. Nor is modernness necessarily a virtue.\nBut readers of this book are urged to always keep in mind the two examples above\\(\\textemdash\\)family income and an interaction term each having minuscule impact even though the tests declared them “significant”\\(\\textemdash\\)in conducting future analyses, or assessing the analysis of others.\nInstead, form a confidence interval for the quantity of interest. Do not just note whether the CI contains 0. Where is the CI’s center? How wide is it, relative to the accuracy you wish to have? Then make your decision regarding importance of the effect on the basis of the overall situation, not by mechanically performing a test."
  },
  {
    "objectID": "PartI.html#sec-compas",
    "href": "PartI.html#sec-compas",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.7 Example: COMPAS dataset",
    "text": "2.7 Example: COMPAS dataset\nRecall that COMPAS is a tool developed to help judges decide on sentences in criminal trials, which it does by predicting whether the defendant is likely to recidivate. As noted earlier, one study found the tool to be biased against African-Americans; the creator of the tool, Northpointe, disagrees.\nLet’s investigate this, by predicting Y = the COMPAS score, with S = race and C = the remaining variables. Does S have much effect on Y?\n\ndata(compas1)\nw &lt;- dsldLinear(compas1,'decile_score','race')\nsummary(w)\n\n$`Summary Coefficients`\n             Covariate     Estimate StandardError       PValue\n1          (Intercept)  12.46413291   1.452859168 0.000000e+00\n2                  age  -0.09404721   0.002497211 0.000000e+00\n3        juv_fel_count   0.39479964   0.073934975 9.303767e-08\n4       juv_misd_count   0.16212898   0.061011256 7.875488e-03\n5      juv_other_count   0.29435177   0.060546864 1.164686e-06\n6         priors_count   0.22714719   0.006509637 0.000000e+00\n7              sexMale  -0.13540331   0.069213068 5.042679e-02\n8    two_year_recidYes   0.82273099   0.062559111 0.000000e+00\n9            raceAsian  -0.68595234   0.393500071 8.129735e-02\n10       raceCaucasian  -0.46795096   0.062339397 6.061818e-14\n11        raceHispanic  -0.93831638   0.101286306 0.000000e+00\n12 raceNative American   0.38640654   0.554347886 4.857734e-01\n13           raceOther  -1.30753368   0.123716368 0.000000e+00\n14           c_jail_in -69.51719068   9.027235193 1.354472e-14\n15          c_jail_out  70.72627748   7.301645508 0.000000e+00\n16      c_offense_date  -4.21691395   1.326098890 1.473059e-03\n17      screening_date  -7.75559525   5.422330875 1.526291e-01\n18          in_custody  -6.31642337   2.419684027 9.042765e-03\n19         out_custody   9.97020948   1.951867970 3.255267e-07\n\n$`Sensitive Factor Level Comparisons`\n                     Factors Compared  Estimates Standard Errors      P-Value\n1            African-American - Asian  0.6859523       0.3935001 8.129735e-02\n2        African-American - Caucasian  0.4679510       0.0623394 6.061818e-14\n3         African-American - Hispanic  0.9383164       0.1012863 0.000000e+00\n4  African-American - Native American -0.3864065       0.5543479 4.857734e-01\n5            African-American - Other  1.3075337       0.1237164 0.000000e+00\n6                   Asian - Caucasian -0.2180014       0.3935940 5.796653e-01\n7                    Asian - Hispanic  0.2523640       0.4015448 5.296877e-01\n8             Asian - Native American -1.0723589       0.6779716 1.137143e-01\n9                       Asian - Other  0.6215813       0.4077123 1.273692e-01\n10               Caucasian - Hispanic  0.4703654       0.1033446 5.328378e-06\n11        Caucasian - Native American -0.8543575       0.5555712 1.240975e-01\n12                  Caucasian - Other  0.8395827       0.1253271 2.096612e-11\n13         Hispanic - Native American -1.3247229       0.5612839 1.826678e-02\n14                   Hispanic - Other  0.3692173       0.1485386 1.293095e-02\n15            Native American - Other  1.6939402       0.5659026 2.759400e-03\n\n\nY, the decile score, is the risk of recidivism; the larger the value of Y, the greater the risk.\nA 95% confidence interval for the mean difference between Black and Caucasian risk scores, holding age, juvenile felony count and so on constant, is 0.4680 \\(\\pm\\) 1.96 x 0.0623 = (0.3459,0.5902). Since the risk of recidivism is given in deciles, the value is 1,2,…,10, we see that the tool does indeed appear to be biased against Black defendants. Though the effect is rather small, around 0.6 of a decile point or less,\nand as usual, one must keep in mind the possibility of unseen confounders, it is still a matter of serious concern. Note that the estimated effect of being Blac, 0.4680, is actually larger than the estimated effect of having one additional juvenile felony count, 0.3948."
  },
  {
    "objectID": "PartI.html#deciding-on-a-set-of-confounders-c",
    "href": "PartI.html#deciding-on-a-set-of-confounders-c",
    "title": "2  Part I: Adjustment for Confounders",
    "section": "2.8 Deciding on a set of confounders C",
    "text": "2.8 Deciding on a set of confounders C\nOne may have specific confounders in mind for a particular analysis, but it is often unclear as to which to use, or for that matter, why not use them all? This section addresses those issues.\nWe will address that second question first: Why not use all our variables, other than Y and S, as confounders? We will see that if we have a large number of variables, there is good reason to select only a subset of potential confounders.\nHow might we do so? We will first recommend some graphical and tabular methods aimed at preliminary exploration toward this end, and then present a dsld function that users may find useful for more systematic selection of confounders.\n\n2.8.1 The problem\nThe German credit dataset, included in the fairml package and thus with dsld, consists of a total of 21 variables:\n\ndata(german.credit)\nnames(german.credit)  \n\n [1] \"Account_status\"           \"Duration\"                \n [3] \"Credit_history\"           \"Purpose\"                 \n [5] \"Credit_amount\"            \"Savings_bonds\"           \n [7] \"Present_employment_since\" \"Installment_rate\"        \n [9] \"Other_debtors_guarantors\" \"Resident_since\"          \n[11] \"Property\"                 \"Age\"                     \n[13] \"Other_installment_plans\"  \"Housing\"                 \n[15] \"Existing_credits\"         \"Job\"                     \n[17] \"People_maintenance_for\"   \"Telephone\"               \n[19] \"Foreign_worker\"           \"Credit_risk\"             \n[21] \"Gender\"                  \n\n\nExcluding Y = credit risk and S = gender, that gives us 19 variables to choose among for our confounders. Which ones should we use?\nTechnically, almost any variable is a confounder. The impact may quite minuscule, but through a long chain of relations among many variables, there will usually be at least some connection, though again possibly very faint.\nWell, then, why not use them all? That is what we’ve done in our earlier examples. But there are several issues to consider not using the full set of variable,s i.e. every variable other than Y and S:\n\nIt may result in overfitting, resulting in large standard errors.\nIt is unwieldy, difficult to interpret. Many treatments of these issues speak of a desire for a “parsimonious” model.\nThere is a concern regarding duplication. It may be that, say, some pair of confounders suffices, and adding further confounders does nothing to further clarify discrimination effects.\n\nConcerning this last point: We do not merely have 19 choices for confounders; we must choose from the set of all possible groups of confounders: singletons, pairs, triplets and so on. There are \\(2^{19}\\) possibilities here, about half a million.\nOur specific purpose here is to find a reasonable set of confounders. In essence, that means find variables C such that C is substantially correlated with both Y and S. One way to approach that is to do separate predictions of Y and S, and see which features turn out to predict both well.\n\n\n2.8.2 Exploratory example: engineering wages\nOne useful tool here is the function dsldFrequencyByS(), which aims to analyze categorical (not numeric) columns by printing out a list of frequencies. Since education level in the svcensus data is categorical, we can call the function as follows:\n\nlibrary(dsld)\ndata(svcensus)\ndsldFrequencyByS(svcensus, \"educ\", \"gender\")\n\n       Frequency of zzzOther Frequency of 14 Frequency of 16\nfemale             0.2068052      0.02098615       0.7722086\nmale               0.2177579      0.04110130       0.7411408\n\n\nThe similar frequency values between men and women suggests not using education as a confounder.\nOn the other hand, we do see more interesting results if we look at occupation instead of education:\n\nlibrary(dsld)\ndata(svcensus)\ndsldFrequencyByS(svcensus, \"occ\", \"gender\")\n\n       Frequency of 102 Frequency of 101 Frequency of 100 Frequency of 141\nfemale        0.3117359        0.2349226        0.3274246       0.04258354\nmale          0.2016862        0.2203267        0.3433671       0.01923330\n       Frequency of 140 Frequency of 106\nfemale       0.02587612       0.05745721\nmale         0.04446055       0.17092610\n\n\nNotice for instance the difference between the proportion of males in occupation 106  versus females in that same occupation–a difference of approximately 11%. The difference in frequencies here is much greater than with the ‘educ’ example, which suggests using occupation as a confounder. Since different occupations correlate with different wages, it is possible that this gender difference in occupations proportions could be affecting the perceived relationship between gender and wage.One might consider conducting formal statistical inference in this comparison, calculating a p-value from a chi-square test. But as we saw in Section 2.6. testing for model fit suffers from the same problems as testing for nonzero effects. At any rate, here we are merely doing an exploratory analysis anyway.\n\n\n2.8.3 Exploratory example: Law school admissions data\nSuppose we are investigating the relationships among the variables LSAT score, GPA and race. One way to visualize these relationships would be through dsldScatterPlot3D:\n\ndata(law.school.admissions)\ndsldScatterPlot3D(law.school.admissions, \n   c('lsat','fam_inc','ugpa'), 'race1', pointSize = 4)\n\n\n Of course, visualizing a 3-dimensional scatter plog is more challenging than viewing the ordinary 2-dimensional kind. But it does enable richer search for relationships. Here are a few trends suggested by the plot:This is a plotly interactive graph. To fully understand this function, the reader should execute the function outside of Quarto, i.e. run the above code directly from R. Try try features such as move, rotate, annotation display and so on.\n\nOn the fam_inc axis, the lowest quintile of family income is mostly populated by Black and Latino students, while the upper two levels are almost entirely made up of white students.\nOn the lsat axis, most of those with a lower score happen to be non-white, across all income levels.\nThe ugpa axis has a similar trend to that of the lsat axis, albeit to a much weaker extent.\n\nOnce again, this analysis is merely exploratory, but the graph lends some credence to claims that family income may confound the relationship between race and LSAT score. Note however, that these graphs do not do much to answer the question of whether, in this case, that relationship is substantial. The formal analysis we did earlier indicates that it is not.\nReturning to the question of whether race have a substantial impact on exam results, we can look at the density plot of LSAT scores against different races.\n\n# requires 'webshot' package\ndata(law.school.admissions)\ndsldDensityByS(law.school.admissions, cName = \"lsat\", sName = \"race1\")\n\n\nEach curve represents the distribution of LSAT scores for one race. The graph suggests the possibility of racial bias in the LSAT. But again, this bias may be confounded.\nRecall that the scatter plot also suggested some trend between family income and race. We can investigate this possible relationship by generating another density plot:\n\ndata(law.school.admissions)\ndsldDensityByS(law.school.admissions, cName = \"fam_inc\", sName = \"race1\")\n\n\n White students have larger peaks at income levels 3, 4, and 5, indicating that a larger proportion of white students are in the higher income brackets than non-white students. Conversely, a larger proportion of black and Latino students occupy the lower income bracket levels.Recall that the fam_inc variable is measured in terms of quintiles, 1 through 5. This is the reason behind the seemingly-odd alignment of, say, the Black and Latino curves.\nIf we want to investigate other potential confounders, we can call dsldConfounders() with ‘race1’ as our sensitive variable.\nThis suggests that family income could be confounding the potential relationship between race and LSAT scores. But, as noted, our earlier analysis indicated that the degree of this relationship is very small.\n\n\n\n\n\n\nAlmost anything is a confounder\n\n\n\nOnce one accounts for links of two variables, then links of links and so on, virtually everything in practice is a confounder to some degree, so it is up to the analyst to decide when a feature is confounding enough to be considered for inclusion in a model.\n\n\n\n\n2.8.4 Variable selection methodology\nThe above graphical and tabular approaches may suffice to determine one’s confounders in some applications. We now present more formal, systematic methods.\nMany, many approaches to this variable selection or feature selection  problem have been proposed over the years. The qeML package includes five of them (see the documentation), and there are myriad others.A nice overview of such methodology, and implementations in R and Python, are given in Parr et al.\nOne widely-used method is permutation, which we will use here. To gauge the importance of a variable V in the context of using some ML algorithm, we do two runs through the data, first with the original dataset, and then with the V column changed through a random shuffling of the values in that column. In that second run, our predictive accuracy should be compromised, and the importance measure is taken to be the proportional increase in error rate. The larger the increase, the more important we deem the variable.\n\n\n2.8.5 The dsldCHunting() function\nThis function serves as an aid to selecting confounders, using the permutation-based computation in the randomForests package.\nIts operation is best described by example, which we will present next. Before reading it, though, recall that we are interested in finding variables that are correlated with, i.e. predict well, both Y and S.\n\n\n2.8.6 Example: Boston mortgage data\nHere is an example, using the mortgage data.\n\ndata(mortgageSE)\ndsldCHunting(mortgageSE,'deny','black')\n\n$impForY\n       p_irat        denpmi      loan_val       hse_inc       ltv_med \n 1.546335e-02  1.474640e-02  1.216851e-02  1.102444e-02  7.679418e-03 \n       single         mcred         ccred         condo      ltv_high \n 1.803870e-03  1.589129e-03  1.541556e-03  1.519193e-03  1.199042e-03 \n     probunmp       selfemp        hischl        pubrec \n 1.085909e-03  9.391036e-04 -4.701694e-05 -3.851059e-04 \n\n$impForS\n     loan_val        p_irat       ltv_med       hse_inc         condo \n 1.270951e-02  1.066277e-02  1.012572e-02  6.074246e-03  5.285745e-03 \n        ccred        single      ltv_high        denpmi        hischl \n 3.063738e-03  2.966525e-03  9.887913e-04  8.359624e-04  3.385738e-04 \n     probunmp        pubrec       selfemp         mcred \n 2.252541e-04  1.573050e-04  4.236975e-05 -6.168608e-05 \n\n$inCommon\n$inCommon[[1]]\ncharacter(0)\n\n$inCommon[[2]]\n[1] \"p_irat\"\n\n$inCommon[[3]]\n[1] \"p_irat\"   \"loan_val\"\n\n$inCommon[[4]]\n[1] \"p_irat\"   \"loan_val\" \"hse_inc\" \n\n$inCommon[[5]]\n[1] \"p_irat\"   \"loan_val\" \"hse_inc\"  \"ltv_med\" \n\n$inCommon[[6]]\n[1] \"p_irat\"   \"loan_val\" \"hse_inc\"  \"ltv_med\" \n\n$inCommon[[7]]\n[1] \"p_irat\"   \"loan_val\" \"hse_inc\"  \"ltv_med\"  \"single\"  \n\n$inCommon[[8]]\n[1] \"p_irat\"   \"loan_val\" \"hse_inc\"  \"ltv_med\"  \"single\"   \"ccred\"   \n\n$inCommon[[9]]\n[1] \"p_irat\"   \"denpmi\"   \"loan_val\" \"hse_inc\"  \"ltv_med\"  \"single\"   \"ccred\"   \n[8] \"condo\"   \n\n$inCommon[[10]]\n[1] \"p_irat\"   \"denpmi\"   \"loan_val\" \"hse_inc\"  \"ltv_med\"  \"single\"   \"ccred\"   \n[8] \"condo\"    \"ltv_high\"\n\n\nThe importance measures are printed out (the ranking is what matters), followed by the “top-i” sets in common.\nFor instance, the variables (excluding S) that predict Y well are first p_irat, then denpmi and so on. In predicting S, excluding Y, the best are p_irat, then loan_val et cetera.\nSince we are focusing on variables that are correlated with both Y and S, we take the intersection. For example, the intersection of the top three Y predictors and top three S predicts is p_irat and loan_val.\nAs usual, there are no magic formulas to use here. The analyst is given a choice. One might use just the top two confounders, or the top three and so on.\n\n\n2.8.7 Example: Employer bias test\nIn the paper “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” (Am. Econ. Rev., Sept. 2004), researcher “test” employers by sending CVs with “white-sounding” and “Black-sounding” names, checking for bias. Did employers call “whites” more often than “Blacks” for an interview? The resulting data is lak in dsld.\n\ndata(lak)\ndim(lak)\n\n[1] 447  63\n\nnames(lak)\n\n [1] \"education\"          \"ofjobs\"             \"yearsexp\"          \n [4] \"honors\"             \"volunteer\"          \"military\"          \n [7] \"empholes\"           \"occupspecific\"      \"occupbroad\"        \n[10] \"workinschool\"       \"email\"              \"computerskills\"    \n[13] \"specialskills\"      \"firstname\"          \"sex\"               \n[16] \"race\"               \"h\"                  \"l\"                 \n[19] \"call\"               \"city\"               \"kind\"              \n[22] \"adid\"               \"fracblack\"          \"fracwhite\"         \n[25] \"lmedhhinc\"          \"fracdropout\"        \"fraccolp\"          \n[28] \"linc\"               \"col\"                \"expminreq\"         \n[31] \"schoolreq\"          \"eoe\"                \"parent_sales\"      \n[34] \"parent_emp\"         \"branch_sales\"       \"branch_emp\"        \n[37] \"fed\"                \"fracblack_empzip\"   \"fracwhite_empzip\"  \n[40] \"lmedhhinc_empzip\"   \"fracdropout_empzip\" \"fraccolp_empzip\"   \n[43] \"linc_empzip\"        \"manager\"            \"supervisor\"        \n[46] \"secretary\"          \"offsupport\"         \"salesrep\"          \n[49] \"retailsales\"        \"req\"                \"expreq\"            \n[52] \"comreq\"             \"educreq\"            \"compreq\"           \n[55] \"orgreq\"             \"manuf\"              \"transcom\"          \n[58] \"bankreal\"           \"trade\"              \"busservice\"        \n[61] \"othservice\"         \"missind\"            \"ownership\"         \n\n\nWe’ll take Y to be call, which records whether the “applicant” received a call back in response to submitting the CV. S will be race, which now leaves us with 63 - 2 = 61 potential confounders!\nActually, it’s even worse. Many rows of the dataset contain missing values (coded NA in R). The version here uses only intact rows, of which there are 447. (The original data had 4870 rows.) A rough rule of thumb commonly cited in statistics is that in regression estimation, the number of features should be less than the square root of the number of data points; by this or almost any other measure, 61 predictors is well beyond the capacity of 447 data points. And, as noted, that number of variables is unwieldy. Let’s see what we can do to reduce that.\n\ndsldCHunting(lak,'call','race',25)\n\n$impForY\n   fraccolp_empzip fracdropout_empzip        linc_empzip   lmedhhinc_empzip \n      5.499015e-03       4.361030e-03       3.922478e-03       2.913287e-03 \n              linc   fracblack_empzip       parent_sales         branch_emp \n      2.625403e-03       2.615064e-03       2.146387e-03       1.521750e-03 \n      branch_sales   fracwhite_empzip           fraccolp            manager \n      1.413845e-03       1.395085e-03       1.381751e-03       1.366411e-03 \n              adid        fracdropout      specialskills          lmedhhinc \n      1.234457e-03       1.220828e-03       1.038361e-03       1.022045e-03 \n           compreq          expminreq         parent_emp           yearsexp \n      8.188585e-04       7.582611e-04       7.056381e-04       6.318586e-04 \n              city          fracwhite      occupspecific          secretary \n      6.044175e-04       5.226365e-04       4.617007e-04       4.005274e-04 \n         fracblack           empholes          ownership              email \n      3.536053e-04       2.633629e-04       2.377489e-04       2.227699e-04 \n       retailsales                  h                eoe               kind \n      1.956400e-04       1.797562e-04       1.778599e-04       1.674892e-04 \n         education          schoolreq                sex             comreq \n      1.176300e-04       7.786616e-05       7.686346e-05       7.409691e-05 \n            expreq           salesrep         supervisor                col \n      6.464075e-05       5.780167e-05       4.137827e-05       4.060328e-05 \n        offsupport             ofjobs             honors         othservice \n      3.919810e-05       3.660207e-05       1.792263e-05       1.586668e-05 \n            orgreq              trade           transcom            missind \n      1.398601e-05       5.812189e-07       0.000000e+00       0.000000e+00 \n        occupbroad              manuf                fed           military \n     -9.468491e-06      -2.887535e-05      -4.110741e-05      -4.166477e-05 \n          bankreal                req            educreq                  l \n     -4.505495e-05      -4.771565e-05      -7.330976e-05      -8.455244e-05 \n         volunteer         busservice     computerskills          firstname \n     -9.965509e-05      -1.080582e-04      -1.092721e-04      -1.179926e-04 \n      workinschool \n     -3.082498e-04 \n\n$impForS\n         firstname               linc          lmedhhinc          fracwhite \n      7.601098e-02       2.917741e-03       2.590713e-03       2.347715e-03 \n            ofjobs     computerskills         othservice           military \n      1.253844e-03       8.314399e-04       4.699709e-04       4.635878e-04 \n         education          fracblack           empholes            compreq \n      2.241179e-04       1.796035e-04       1.269505e-04       3.039656e-05 \n         ownership            missind           transcom               city \n      1.139559e-05       0.000000e+00      -5.941901e-05      -6.025754e-05 \n           manager              manuf        fracdropout                sex \n     -1.141520e-04      -1.360799e-04      -1.711292e-04      -1.956709e-04 \n              kind                col        retailsales              email \n     -2.300770e-04      -2.370655e-04      -2.746826e-04      -2.777165e-04 \n            comreq                  l          schoolreq           fraccolp \n     -2.850064e-04      -2.939751e-04      -3.657418e-04      -3.703502e-04 \n          bankreal            educreq           salesrep         offsupport \n     -3.835777e-04      -4.139371e-04      -4.556876e-04      -4.669090e-04 \n               req             honors             expreq                fed \n     -4.723912e-04      -5.165723e-04      -5.265490e-04      -5.457982e-04 \n         secretary      specialskills             orgreq         occupbroad \n     -5.631163e-04      -5.741544e-04      -6.948797e-04      -7.628087e-04 \n        supervisor         busservice              trade          volunteer \n     -7.656347e-04      -8.492082e-04      -8.692304e-04      -1.080123e-03 \n   fraccolp_empzip                  h       workinschool                eoe \n     -1.158906e-03      -1.160250e-03      -1.604002e-03      -1.730779e-03 \n         expminreq   fracwhite_empzip      occupspecific fracdropout_empzip \n     -2.416712e-03      -2.526489e-03      -2.579276e-03      -2.624324e-03 \n          yearsexp         parent_emp   lmedhhinc_empzip       branch_sales \n     -2.812154e-03      -3.308079e-03      -3.445445e-03      -3.737839e-03 \n       linc_empzip         branch_emp       parent_sales   fracblack_empzip \n     -4.155636e-03      -4.244323e-03      -4.428632e-03      -4.480281e-03 \n              adid \n     -6.896452e-03 \n\n$inCommon\n$inCommon[[1]]\ncharacter(0)\n\n$inCommon[[2]]\ncharacter(0)\n\n$inCommon[[3]]\ncharacter(0)\n\n$inCommon[[4]]\ncharacter(0)\n\n$inCommon[[5]]\n[1] \"linc\"\n\n$inCommon[[6]]\n[1] \"linc\"\n\n$inCommon[[7]]\n[1] \"linc\"\n\n$inCommon[[8]]\n[1] \"linc\"\n\n$inCommon[[9]]\n[1] \"linc\"\n\n$inCommon[[10]]\n[1] \"linc\"\n\n$inCommon[[11]]\n[1] \"linc\"\n\n$inCommon[[12]]\n[1] \"linc\"\n\n$inCommon[[13]]\n[1] \"linc\"\n\n$inCommon[[14]]\n[1] \"linc\"\n\n$inCommon[[15]]\n[1] \"linc\"\n\n$inCommon[[16]]\n[1] \"linc\"      \"lmedhhinc\"\n\n$inCommon[[17]]\n[1] \"linc\"      \"manager\"   \"lmedhhinc\" \"compreq\"  \n\n$inCommon[[18]]\n[1] \"linc\"      \"manager\"   \"lmedhhinc\" \"compreq\"  \n\n$inCommon[[19]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n\n$inCommon[[20]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n\n$inCommon[[21]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n[6] \"city\"       \n\n$inCommon[[22]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n[6] \"city\"        \"fracwhite\"  \n\n$inCommon[[23]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n[6] \"city\"        \"fracwhite\"  \n\n$inCommon[[24]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n[6] \"city\"        \"fracwhite\"  \n\n$inCommon[[25]]\n[1] \"linc\"        \"manager\"     \"fracdropout\" \"lmedhhinc\"   \"compreq\"    \n[6] \"city\"        \"fracwhite\"   \"fracblack\"  \n\n\nWe are seeking variables that are correlated both with Y and S, yet there appear to be many that correlate with just one of these. That’s to be expected for datasets having a large number pf variables. But still, in the end we see a few that meet our goals."
  },
  {
    "objectID": "PartII.html#goals",
    "href": "PartII.html#goals",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.1 Goals",
    "text": "3.1 Goals\nThere are two main aspects of fair machine learning:\n\nMeasuring unfairness: A number of measures have been proposed.\nReducing unfairness: For a given ML algorithm, how can we ameliorate its unfairness, yet still maintain an acceptable utility (predictive power) level?\n\nIn our earlier COMPAS example: Is the risk assessment tool biased against African-Americans? And if so, how can we reduce that bias while still maintaining good predictive ability?"
  },
  {
    "objectID": "PartII.html#comparison-to-part-i",
    "href": "PartII.html#comparison-to-part-i",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.2 Comparison to Part I",
    "text": "3.2 Comparison to Part I\nFirst, recall our notation: As before with C, think of X as, at first, consisting of all variables other than Y and S, but then selecting some of X as O, with X then being all variables but Y, S and O.\n\nY: outcome variable, to be predicted\nS: sensitive variable\nO: proxy variables\nX: other variables to be used to predict Y\n\nWe wish to predict Y from X and O, omitting S, but with concern that we may be indirectly using S via O. Contrast this from our material in Part I:\n\nIn Part I, we fit models for predicting Y, but with the goal of using such models to assess the effect of S on Y; we were not interested in actually predicting Y. We included S in our models, but wished to find variables C that were correlated with both Y and S, so as to avoid distorting our look at the impact of S on Y. Any X variable unrelated to Y was not of interest.\nHere in Part II, prediction of Y is our central goal, rather than effect assessment. We will omit S, relying our prediction fully on X and partly on O. The variables O are related to S; the stronger the relation of an O variable to S, the less weight we will put on that variable in predicting Y.\n\nWe will describe some common measures of unfairness shortly. But first, how do we choose the O variables?\n\n3.2.1 Deciding proxies\nAs with choosing confounders in Part I, the analyst may simply choose  all possible candidate O variables.\nthe proxies based on his/her domain expertise. But a more formal approach may involve correlation. The function dsldOHunting calculates the correlations between S andIt should be kept in mind that, as with any statistic, all utility and fairness measures are subject to sampling variation.\nHere’s an example, using the COMPAS data:\n\nlibrary(dsld)\n\nLoading required package: fairml\n\n\nLoading required package: regtools\n\n\nLoading required package: FNN\n\n\n\n\n\n\n\n*********************\n\n\n\nLatest version of regtools at GitHub.com/matloff\n\n\nType ?regtools to see function list by category\n\n\nLoading required package: qeML\n\n\nLoading required package: rmarkdown\n\n\nLoading required package: tufte\n\n\n\n\n\n\n\n*********************\n\n\n\n  Navigating qeML:\n\n      Type vignette(\"Quick_Start\") for a quick overview!\n\n      Type vignette(\"Function_List\") for a categorized function list\n\n      Type vignette(\"ML_Overview\") for an introduction to machine learning\n\n\n\nAttaching package: 'qeML'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    evalr\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndata(compas1) \ncmp &lt;- compas1[,-3]  # omit decile; we are developing our own risk tool\ndsldOHunting(cmp,'two_year_recid','race')\n\n                               age juv_fel_count juv_misd_count juv_other_count\nrace.African-American -0.156957517    0.10966300    0.114479498      0.07478931\nrace.Asian             0.016385005   -0.01239908   -0.005952701     -0.01014768\nrace.Caucasian         0.147628857   -0.08028929   -0.090027168     -0.04101560\nrace.Hispanic          0.022247056   -0.03052358   -0.030760605     -0.02561435\nrace.Native American   0.002329938    0.03159615   -0.011713229      0.01229258\nrace.Other             0.002437568   -0.03832635   -0.020729116     -0.04670631\n                      priors_count   sex.Female     sex.Male    c_jail_in\nrace.African-American   0.17509146 -0.041459873  0.041459873  0.002270144\nrace.Asian             -0.03073693 -0.021694502  0.021694502 -0.004670234\nrace.Caucasian         -0.10017594  0.068682539 -0.068682539  0.002112483\nrace.Hispanic          -0.06814588 -0.026131928  0.026131928 -0.006687662\nrace.Native American    0.02388670 -0.006505234  0.006505234 -0.004382357\nrace.Other             -0.08725931 -0.012952574  0.012952574  0.001166723\n                        c_jail_out c_offense_date screening_date   in_custody\nrace.African-American  0.012784849  -0.0084427004   -0.004593993  0.060971397\nrace.Asian            -0.004725654  -0.0006422783   -0.004131066 -0.021151109\nrace.Caucasian        -0.005997440   0.0117472861    0.009717379 -0.027889553\nrace.Hispanic         -0.009454646  -0.0080839293   -0.004909945 -0.033209571\nrace.Native American  -0.004711864  -0.0083578408   -0.003890418 -0.005750257\nrace.Other            -0.001456317   0.0058788162   -0.002179469 -0.027193953\n                       out_custody\nrace.African-American  0.071601855\nrace.Asian            -0.023006228\nrace.Caucasian        -0.036894524\nrace.Hispanic         -0.036660474\nrace.Native American  -0.003430048\nrace.Other            -0.027359200\n\n\nThe output here suggests possibly using, say, the age and priors_count variables as proxies.\nThe function does not use the classic Pearson product moment correlation here, opting instead for Kendall’s tau correlation. Both are widely-used, and both take on values in [-1,], but while Pearson is geared toward continuous numeric variables, Kendall is also usable for  binary or ordinal integer-valued variables.Tau is defined in terms of concordances and discordances. Say we look at height and weight, and compare two people. If one of the people is both taller and heavier than the other, that is a concordance. If on the other hand, one is shorter but heavier than the other, that is a discordance. We consider all possible pairs of people in our dataset, then sets tau to the difference in concordance and discordance counts, divided by the number of pairs."
  },
  {
    "objectID": "PartII.html#measuring-utility",
    "href": "PartII.html#measuring-utility",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.3 Measuring utility",
    "text": "3.3 Measuring utility\nUtility in ML classification algorithms in general, and both utility and fairness in the fair ML classification realm, often (but far from always) make use of quantitaties like False Positive Rate (FPR). So, let’s start by defining these rates.\n\n\n\n\n\n\nThe famous rates FPR etc.\n\n\n\nThese are conditional probabilities, but it’s important not to confuse the event with the condition.\nConsider binary Y classification problems, where we label Y is either positive (e.g. patient has the disease) or negative (e.g. patient does not have the disease).\nAfter we fit our ML tool to predict Y, we use it for prediction. Consider a long period of time in which we do such predictions. During that time, define the following counts:\n\nFP: Count of the number of times we predict Y to be positive and actually it’s negative.\nFN: Count of the number of times we predict Y to be negative and actually it’s positive.\nTP: Count of the number of times we predict Y to be positive and actually it’s positive.\nTN: Count of the number of times we predict Y to be negative and actually it’s negative.\n\nThen some key rates are: \n\nFPR: FP / (FP + TN) = P(guess Y positive | Y actually is negative)\nTPR: TP / (TP + FN) = P(guess Y positive | Y actually is positive)\nFNR: FN / (TP + FN) = P(guess Y negative | Y actually is positive)\nTNR: TN / (FP + TN) = P(guess Y negative | Y actuually is negative)\n\nSo for instance, FPR is the proportion of time we guess positive, among those times in which Y is actually negative. Two other common terms:\n\nrecall: same as TPR\nsensitivity: same as TPR\nprecision: P(Y is actually positive | we guess Y is positive)= (TP + FN) / (TP + FP)\n\n\n\nIt is standard to guess Y = 1 if the probability of that event is at least 0.5. But other thresholds can be used, with TPR and FPR varying as we vary the threshold. The ROC curve is the resulting graph of TPR vs. FPR; see qeROC in the qeML package.A simple but quite common measure of utility in binary classification problems is the overall misclassification probability  of misclassification; qeML predictive functions report this in the testAcc component of the functions’ return object. There are many, many other measures.The F1-score has recently been especially popular in the ML research world. It is defined as the harmonic mean of precision and recall, and is thought to be especially useful in applications in which one class or the other is very rare.\nFor whatever reason, ML research has tended to focus on that binary Y case, and there are no analogous acronyms for numeric Y. Accuracy of the latter is handled simply as Mean Squared Prediction Error (MSPE), the average squared difference between predicted and actual Y value, or Mean Absolute Prediction Error (MAPE)."
  },
  {
    "objectID": "PartII.html#measuring-unfairness",
    "href": "PartII.html#measuring-unfairness",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.4 Measuring unfairness",
    "text": "3.4 Measuring unfairness\nMany unfairness criteria have been proposed. We preseent a few of them in this section.  It should be kept in mind that, just as there is no single ML algorithm that predicts the best in all applications, one’s choice of fairness measure also will depend on the given application.See the Xiang and Raji for an excellent analysis of the legal implications of various fairness measures.\n\n3.4.1 S-Correlation\nA direct way to measure where Y and S are still related in spite of physically omitting the latter is to compute the correlation between predicted Y, to be denoted \\(\\hat{Y}\\) and S. As noted earlier, we use Kendall’s Tau correlation here.\n For instance, let’s consider our mortgage example, say with k-Nearest Neighbors as our ML prediction tool:Here and below, to keep things simple, we will not use a holdout set.\n\nlibrary(dsld)\nlibrary(qeML)\nz &lt;- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes') \n# look at the fitted model's probability of recidivism,\n# i.e. regression estimates\nprobs &lt;- z$regests\n# the variable 'race' is an R factor, need a numeric\nblack &lt;- ifelse(cmp$race=='African-American',1,0)\ncor(black,probs,method='kendall')\n\n[1] 0.2343988\n\n\nThat’s a pretty substantial correlation, definitely a cause for concern that our ML analysis here is unfair. Of course, it’s not the algorithm itself’s fault, but we must find a way to mitigate the problem.\n An advantage of the S-Correlation measure is that it can also be used in non-classification problems, say predicting wage income, and take age as our sensitive variable S:Prediction of income might be of interest in, say, a marketing context. Actually, the field of marketing has been the subject of much concern in fair ML; e.g. see FWA.\n\ndata(svcensus)\nz &lt;- qeKNN(svcensus,'wageinc',holdout=NULL)\ncor(z$regests,svcensus$age,method='kendall') \n\n[1] 0.225566\n\n\nSo, again, our ML tool seems to be biased, this time in terms of age.\n\n\n3.4.2 Demographic Parity\nThe criterion for demographic parity is that the same proportion of each sub-group within the sensitive feature is classified at equal rates for each of the possible outcomes\nFor example, let’s consider the COMPAS dataset again. Demographic Parity would require\nP(predict recidivate | Black) = P(predict recidivate | white),\ni.e. that the quanitty\n(TP+FP) / (TP+FP+TN+FN)\nhas the same value for each race.\nBelow is an example using the COMPAS data:\n\nz &lt;- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes')\n# determine which rows were for Black applicants, which not\nBlackRows &lt;- which(cmp$race == 'African-American')\nNonblackRows &lt;- setdiff(1:nrow(cmp),BlackRows)  # all the others\n# regests, the output of qeKNN, is the vector of fitted probabilities\n# (recall that for the Y = 0,1 case, the mean reduces to the probability\n# of a 1)\nBlackProbs &lt;- z$regests[BlackRows]\nNonblackProbs &lt;- z$regests[NonblackRows]\n# if a probability is &gt; 0.5, we will guess Y = 1, otherwise guess Y = 0;\n# conveniently, that's same as rounding to the nearest integer\nBlackYhats &lt;- round(BlackProbs)\nNonblackYhats &lt;- round(NonblackProbs)\n# again, recall that the mean of a bunch of 0s and 1s is the proportion\n# of 1s, i.e. the probability of a 1\nmean(BlackYhats)\n\n[1] 0.5073104\n\nmean(NonblackYhats)\n\n[1] 0.2724777\n\n\nThat’s quite a difference! Overall, our ML model predicts about 51% of Black defendants to recidivate,j versus than 27% for non-Blacks.\nHowever, such a criterion is generally considered too coarse, since it doesn’t account for possible differences in qualifications between the two groups. In other words, one must take confounders into account, as we did in Part I.\n\n\n3.4.3 Equalized Odds\nThis criterion takes a retrospective view, asking in the case of COMPAS:\n\nAmong those who recidivate, what proportion of them had been predicted to do so? And, does that proportion vary by race?\n\nIf the answer to that second question is No, we say our prediction tool satisfies the Equalized Odds criterion.\nSo, Equalized Odds requires the quantity\nTP / (TP+FN)\nto be the same for each sensitive group.\n\n\n3.4.4 The fairness package\nThis package calculates and graphically displays a wide variety of fairness criteria. For instance, let’s use it to evaluate the Equalized Odds criterion in the above COMPAS example.\n\nlibrary(fairness)\n\n\nAttaching package: 'fairness'\n\n\nThe following object is masked from 'package:fairml':\n\n    compas\n\nequal_odds(cmp,'two_year_recid','race',probs=z$regests)\n\n$Metric\n               African-American Asian    Caucasian    Hispanic Native American\nSensitivity           0.7536694     0    0.5943627   0.4810811       0.5714286\nEqualized odds        1.0000000     0    0.7886253   0.6383184       0.7581952\nGroup size         2941.0000000    28 2055.0000000 501.0000000      14.0000000\n                     Other\nSensitivity      0.5344828\nEqualized odds   0.7091740\nGroup size     316.0000000\n\n$Metric_plot\n\n\n\n\n\n\n$Probability_plot\n\n\n\n\n\nTaking African-Americans as the base, we see that the Equalized Odds criterion was not met, even approximately. Nor was Demographic Parity."
  },
  {
    "objectID": "PartII.html#remedies",
    "href": "PartII.html#remedies",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.5 Remedies",
    "text": "3.5 Remedies\nHaving established that ML prediction models can be biased against certain sensitive groups, what remedies are available? The dsld package includes a few of these, presented in this section. Of course, all of them recognize a basic principle:\n\n\n\n\n\n\nThe Fairness-Utility Tradefoff\n\n\n\nOf course, nothing comes for free: the inherent tradeoff of increasing fairness is reduced utility (reduced predictive power over the dataset). Thus, a means of balancing this tradeoff between fairness and utility becomes essential in any future implementations of machine learning.\n\n\nAny algorithm for ameliorating unfairness will thus include one or more parameters that one can use to achieve a desired level of compromise between fairness and utility. The parameters essentially allow us to “dial” the weight that our proxies will play in predicting Y; lighter weight means more fairness but poorer utility, and vice versa.\nOur context will be:\n\nDue to legal requirements or simply a desire for fairness, we will omit S from all analyses, other than for fairness assessment of our derived prediction tool.\nBut we are concerned about the impact of proxies, and have chosen a set of variables O to play this role.\nWe have chosen fairness and utility measures with which we will choose a desired point in the Fairness-Utility Tradefoff.\n\nMany, many fair ML algorithms have been proposed, most of which are technically complex. The ones we present here have been chosen (a) for their technical simplicity and (b) availability as R packages. We will begin with the simplest algorithms (which have been developed by one of the authors of this book).\nLet’s take as a running example the COMPAS data, predicting recidivism, with age and prior convictions count as proxies. To illustrate prediction, we will predict the first case in the dataset:\n\nnewx &lt;- cmp[1,-(7:8)]  # just X and O, not Y and S"
  },
  {
    "objectID": "PartII.html#dsldqefairrf",
    "href": "PartII.html#dsldqefairrf",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.6 dsldQeFairRF",
    "text": "3.6 dsldQeFairRF\nThis function fits an RF model, but with deweighting of the proxies.\n\nz &lt;- dsldQeFairRF(cmp,'two_year_recid','race',\n   list(age=0.2,priors_count=0.1),yesYVal='Yes')\n\nWarning: Split select weights used. Variable importance measures are only comparable for variables with equal weights.\n\nz$corrs\n\nAfrican-American            Asian        Caucasian         Hispanic \n      0.21366424       0.15610522       0.16840232       0.20264208 \n Native American            Other \n      0.01015095       0.14011174 \n\npredict(z,newx)\n\n$predClasses\n[1] \"No\"\n\n$probs\n            No       Yes\n[1,] 0.6555973 0.3444027\n\n\nRecall that in RFs, each tree will use a different random ordering of the X and O variables. At any given node in a tree, a variable is chosen at random to set up a possible split point. But here we are specifying that the age and priors count variables be used only 20% and 10% as often as other variables, in order to reduce their impact.\nYet the S-Correlation valuea are still substantial. We need to give the O variables even smaller weights or possibly include additional variables in our O set."
  },
  {
    "objectID": "PartII.html#dsldqefairknn",
    "href": "PartII.html#dsldqefairknn",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.7 dsldQeFairKNN",
    "text": "3.7 dsldQeFairKNN\nRemember, the common theme here is reducing the role played in prediction by the proxies O. How might this be done with k-NN?\nAn easy answer is to weight the distance metric. Ordinarily, if we move 3.2 meters to the right and then 1.1 meters forward, the distance between our new point and our original one is\n\\(\\sqrt{3.2^2+1.1^2} = 3.38\\)\nThat puts equal weight in the left-right direction and the forward-back direction, which makes sense for geometric distance. But in data prediction, we can use different weights. In prediction wage income in the Census data, say, we can place large weight on age and occupation, and less weight on education. The above call would change to:\n\nz &lt;- dsldQeFairKNN(cmp,'two_year_recid','race',\n   list(age=0.2,priors_count=0.1),yesYVal='Yes')\nz$corrs\npredict(z,newx)"
  },
  {
    "objectID": "PartII.html#remedies-based-on-shrunken-linear-models",
    "href": "PartII.html#remedies-based-on-shrunken-linear-models",
    "title": "3  Part II: Discovering/Mitigating Bias in Machine Learning",
    "section": "3.8 Remedies based on “shrunken” linear models",
    "text": "3.8 Remedies based on “shrunken” linear models\nOne of the most striking advances in modern statistics was the discovery that classical estimators tend to be “too big,” and that one may improve accuracy by “shrinking” them. Here is the intuition:\nConsider the example in Section 2.7. The vector of estimated regression coefficients was\n(12.46,-0.94,0.39,0.16,…)\nWe might (crudely) shrink this vector by multiplying by a factor of, say, 0.5, yielding\n(6.23,-0.47,0.20,0.08,…)\nThe actual shrinkage mechanisms used in the regression context are much more complex than simply multiplying every component of the vector by the same constant, but this is the basic principle.\nWhy might this odd action be helpful? Shrinking introduces a bias, but reduces variance (roughly speaking, smaller variables vary less). If outliers (extreme values, not errors) are common in the data, these result in large estimator variance, possibly so much that shrinkage’s reduction in variance overwhelms our increase in bias.\nSome readers may have heard of ridge regression and the LASSO, both of which impose shrinkage. For our fair ML context, though we wish to perform our shrinkage focusing on only some of the estimated regression coefficients, specifically those corresponding to our proxies. To be sure, it should be noted that non-proxy coefficients are affected too, but the main effects will be on the proxies.\nNote that these estimators also have the potential ancillary benefit obtained from shrinkage in general, i.e. better utility.\n\n3.8.1 The dsldFgrrm function\nThis is a wrapper to a corresponding function in the fairml package, based on a paper by the authors of the package.\nLet’s see what it does with the COMPAS data:\n\ndsldFgrrm(cmp,'two_year_recid','race',unfairness=0.1,family='binomial')\ndsldFgrrm(cmp,'two_year_recid','race',unfairness=0.01,family='binomial')\n\nThe unfairness argument is a number in (0,1]. The smaller the value, the fairer the fit. The first value, 0.1, gave an estimate coefficient for Caucasian of -0.73, while using 0.01 reduced this to -0.15. (Note that the base for the dummy variables is apparently African-American.] But both values are small relative some of the other coefficients."
  },
  {
    "objectID": "Appendices.html",
    "href": "Appendices.html",
    "title": "4  Author Bios",
    "section": "",
    "text": "5 Appendices"
  },
  {
    "objectID": "Appendices.html#norman-matloff",
    "href": "Appendices.html#norman-matloff",
    "title": "4  Author Bios",
    "section": "Norman Matloff",
    "text": "Norman Matloff\nis Professor Emeritus of Computer Science, and was  formerly a Professor of Statistics at that university. His participation in this project is informed in part by his experience serving as an expert witness in in a number of litigation cases involving discrimination. His work has been recognized in various forms, including the university-wide Distinguished Teaching Award, Outstanding Adviser Award, and Distinguished Public Service Award. His book, Statistical Regression and Classification: from Linear Models to Machine Learning was the 2017 recipient of the Ziegal Award, given by the statistics journal Technometrics."
  },
  {
    "objectID": "Appendices.html#taha-abdullah",
    "href": "Appendices.html#taha-abdullah",
    "title": "4  Author Bios",
    "section": "Taha Abdullah",
    "text": "Taha Abdullah\nis studying for a B.S. in Computer Science at University of California, Davis. He has a keen interest in pursuing a career in Software Engineering."
  },
  {
    "objectID": "Appendices.html#arjun-ashok",
    "href": "Appendices.html#arjun-ashok",
    "title": "4  Author Bios",
    "section": "Arjun Ashok",
    "text": "Arjun Ashok\nis currently a sophomore undergraduate at UC Davis  double-majoring in Computer Science and Statistics with a minor in Economics. As indicated by his past research and publications, he’s primarily interested in the R&D side of machine learning, although he’s intent on taking his knowledge into real-world applications for developmental purposes."
  },
  {
    "objectID": "Appendices.html#shubhada-martha",
    "href": "Appendices.html#shubhada-martha",
    "title": "4  Author Bios",
    "section": "Shubhada Martha",
    "text": "Shubhada Martha\nis a third year Computer Science student pursuing her undergraduate studies at UC Davis. She’s passionate about ethics in technology in the areas of full stack development and AI and Machine Learning. During her  spare time, she likes to read spiritual and self-improvement books and paint."
  },
  {
    "objectID": "Appendices.html#aditya-mittal",
    "href": "Appendices.html#aditya-mittal",
    "title": "4  Author Bios",
    "section": "Aditya Mittal",
    "text": "Aditya Mittal\n is pursuing a B.S. in Statistics with a minor in Computer Science at University of California, Davis. During the summer, he is employed as a Business Analyst Intern at Cisco and will be returning in summer 2024 for another round. He is interested in pursuing a career in machine learning/software engineering. Fun fact: He moved to the U.S. in 2014 from Mumbai, India."
  },
  {
    "objectID": "Appendices.html#billy-ouattara",
    "href": "Appendices.html#billy-ouattara",
    "title": "4  Author Bios",
    "section": "Billy Ouattara",
    "text": "Billy Ouattara\nis a senior Computer Science and Engineering student, His academic journey has been defined by a strong commitment to ethical considerations in the development and deployment of ML/AI models. This dedication is one of the key reasons he eagerly participated in this project."
  },
  {
    "objectID": "Appendices.html#jonathan-tran",
    "href": "Appendices.html#jonathan-tran",
    "title": "4  Author Bios",
    "section": "Jonathan Tran",
    "text": "Jonathan Tran\n is studying computer science at UC Davis, with an interest in machine learning and large language models."
  },
  {
    "objectID": "Appendices.html#brandon-zarate",
    "href": "Appendices.html#brandon-zarate",
    "title": "4  Author Bios",
    "section": "Brandon Zarate",
    "text": "Brandon Zarate\nis a senior student at UC Davis studying Computer Science."
  },
  {
    "objectID": "Appendices.html#appendix-a-standard-errorstextemdashstatistical-inference-in-a-nutshell",
    "href": "Appendices.html#appendix-a-standard-errorstextemdashstatistical-inference-in-a-nutshell",
    "title": "4  Author Bios",
    "section": "5.1 Appendix A: Standard Errors\\(\\textemdash\\)Statistical Inference in a Nutshell",
    "text": "5.1 Appendix A: Standard Errors\\(\\textemdash\\)Statistical Inference in a Nutshell\nSay I wish to find the mean weight \\(\\mu\\) of all students at the University of California, Davis. It would be infeasible to measure them all, so I take a random sample of 100 students. The mean weight of those 100 students, denoted \\(\\bar{X}\\), is an estimate of \\(\\mu\\).\nI know that \\(\\bar{X}\\) will have some unknowm amount of error as an estimate of \\(\\mu\\). One measure of this is its standard error (SE). What is this?\nRemember, we took a random sample of students. Different samples will have different values of \\(\\bar{X}\\), i.e. \\(\\bar{X}\\) will have sampling variation. Then the SE of \\(\\bar{X}\\) is defined to be the standard deviation of all possible \\(\\bar{X}\\) values, from all possible samples.\nThe smaller the SE is, the more confident we are that the \\(\\bar{X}\\) from  our particular sample is pretty accurate. Of course, we can’t be completely sure, but we can quantify it probabilistically:Meaning of that 95% probability figure: Think of all possible samples. Each one has an \\(\\bar{X}\\) value, and an SE value. So, each possible sample, there is a different confidence interval (CI). 95% of those CIs will contain the true \\(\\mu\\).\n\nAn approximate 95% confidence interval for \\(\\mu\\), based on \\(\\bar{X}\\), is \\(\\bar{X} \\pm 1.96 \\times SE\\).\n\nThe reader may have heard of the Central Limit Theorem, which says that the sum of many random variables, itself a random variable, has an approximately normal distribution. This is the case for most classical statistical estimators, such as estimated coefficients in linear and generalized linear models. Hence we can find confidence intervals as above, taking the estimator plus and minus 1.96 times the SE of the estimator.\nCIs are generally taught together with hypothesis testing or significance testing, which in many cases uses standard errors.\nSection 2.6 reviews the notion, and discusses problems with this kind of analysis."
  },
  {
    "objectID": "Appendices.html#sec-causal",
    "href": "Appendices.html#sec-causal",
    "title": "4  Author Bios",
    "section": "5.2 Appendix B: A Note on Causal Inference",
    "text": "5.2 Appendix B: A Note on Causal Inference\nAn observational study is, in essence, one that is not planned. For instance, say we are comparing an old and a new drug for hypertension. Suppose that, unknown to us, the new medication does well on younger patients but not on older ones. Say the nature of the data collection process results in disproportionately sampling older patients, but our data itself does not include patient ages. This could unfairly make the new drug appear ineffective.\nIn a randomized clinical trial (RCTs), we would randomly assign treatments to patients, so there would likely be no strong imbalance in age distribution in the two drug groups. So, even if our data still did not record patient age, there would be no age bias in our analysis.\nIn many cases, RCTs are infeasible or impossible (we cannot “assign” race). Causal inference CI can be viewed both as a way to deal with unseen variables in observational studies, and in its graphical forms, as a descriptive alternative to traditional statistical relational analysis. Though CI takes on many forms, our discussion here will focus on directed acyclic graphs (DAGs).\n\n5.2.1 DAGS\nIn a DAG, an arrow from A to B signifies that A “causes” B.\nFor instance, a graphical description of a situation in which older people (say who came of age before anti-smoking campaigns) might be more likely to smoke, and for other reasons may be more likely to develop cancer even if they are nonsmokers. A graphical description would be\n\nHere is an example the bnlearn package on our svcensus dataset:\n   library(bnlearn)\n   library(dsld)\n   data(svcensus)\n   svcensus$wageinc &lt;- as.numeric(svcensus$wageinc)\n   svcensus$wkswrkd &lt;- as.numeric(svcensus$wkswrkd)\n   svcdag &lt;- iamb(svcensus)  # default argument values\n   plot(svcdag)\n\nThere are a couple of undirected arcs, e.g. between gender and  occupation–found to be relations, but with indeterminate direction, using available data.Though again we omit a detailed treatment of CI DAGs here, we note that that field defines a confounder of two variables as a variable that is an “ancestor” to both, education being a confounder for income and occupation.\nMore interesting, though, is the lack of a causal arc from gender to wage income. This is in stark contrast to what dsldLinear suggests:\n\nlibrary(dsld)\n\nLoading required package: fairml\n\n\nLoading required package: regtools\n\n\nLoading required package: FNN\n\n\n\n\n\n\n\n*********************\n\n\n\nLatest version of regtools at GitHub.com/matloff\n\n\nType ?regtools to see function list by category\n\n\nLoading required package: qeML\n\n\nLoading required package: rmarkdown\n\n\nLoading required package: tufte\n\n\n\n\n\n\n\n*********************\n\n\n\n  Navigating qeML:\n\n      Type vignette(\"Quick_Start\") for a quick overview!\n\n      Type vignette(\"Function_List\") for a categorized function list\n\n      Type vignette(\"ML_Overview\") for an introduction to machine learning\n\n\n\nAttaching package: 'qeML'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    evalr\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndata(svcensus)\nw &lt;- dsldLinear(svcensus,'wageinc','gender',interactions=FALSE)\ncoef(w)\n\n$gender\n (Intercept)          age       educ16 educzzzOther       occ101       occ102 \n -21264.9056     469.8373    7597.2521  -14167.3888    1838.5510   13688.3012 \n      occ106       occ140       occ141      wkswrkd   gendermale \n   1380.0254   11732.5837   10791.0158    1323.1999    8595.5831 \n\n\nHere, even correcting for age, education and so on, we still find a substantial gender pay gap. Gender does seem to matter, on its own.\nIt should be noted that if the bnlearn function hc is used instead of iamb (not shown), there is indeed a link from ‘gender’ to ‘wageinc’, but apparently at the cost of other anomalies. For example, there is no arc, or path of arcs, from ‘wkswrkd’ to ‘wageinc’.\nCI DAGs are of course quite visually appealing, and many people find causal concepts helpful in their thoughts processes regarding the data. But as pointed out by the developers of such methodology, the graphs can be very misleading. Using DAGs effectively requires deep understanding of the concepts. A treatment of these concepts is well beyond the scope of this course, but we urge users and consumers of CI to keep the following points in mind:\n\nThe definition of cause is of course central to proper use of CI DAGs. It does not mean what one ordinarily thinks of as causal (“The sidewalk was icy, so the man slipped and fell”), and it can become quite complex; “wrapping one’s head around” the definition can be a challenge even for experienced probabilistic modelers.\nUnseen variables, such as age in the above example, can be just as harmful in CI contexts as in observational data.\nCausal analysis is just as much at the mercy of sample size as is classical statistical analysis, indeed even more so, since the causal version in effect estimates many more parameters. Absence of a link may be due to insufficient data, and nonlinks at the population level may appear as “links” in our graph by random chance in small datasets.\nMost important, DAGs have a highly concerning uniqueness problem:\n\n\n\n\n\n\n\nDAGS are not unique\n\n\n\n\nMost important, for any given dataset, there is typically no unique DAG:  DAG. Typically many DAGs can fit the dataset equally well, and they can be in conflict with each other. One DAG might say that A causes B, while another, equally credible in terms of data fit, might have B causing A, while a third says A and B are independent.\nM. Scutari, author of bnlearn, shows an example in his Cambridge lectures:\n\nThe nonuniqueness is of course very concerning. In recent years, there has been much handwringing in science about the Replication Crisis, in which scientists try to replicate the findings of an earlier published paper, and discover serious problems with the earlier results. Imagine the replication problems that can arise if authors of a paper present just one of many equally-credible DAGs, as is usually the case.\n\n\n\nCMU professor C. Shalizi has bluntly noted that in cases of nonuniqueness, presenting just one DAG without mentioning the equivalent ones, “…[which] is often what people seem to do,” might be viewed as “lying by omission.” Recognizing the nonuniqueness is “simply a matter of scientific honesty.”DAGs can indeed lead to valuable insight, but can be misleading as well. A healthy skepticism and a solid understanding of the concepts are mandatory for avoiding misleading or invalid results."
  },
  {
    "objectID": "Appendices.html#sec-boot",
    "href": "Appendices.html#sec-boot",
    "title": "4  Author Bios",
    "section": "5.3 Appendix C: Standard Errors via the Bootstrap",
    "text": "5.3 Appendix C: Standard Errors via the Bootstrap"
  },
  {
    "objectID": "Appendices.html#appendix-d-installing-the-software",
    "href": "Appendices.html#appendix-d-installing-the-software",
    "title": "4  Author Bios",
    "section": "5.4 Appendix D: Installing the Software",
    "text": "5.4 Appendix D: Installing the Software"
  },
  {
    "objectID": "Appendices.html#python-interface",
    "href": "Appendices.html#python-interface",
    "title": "4  Author Bios",
    "section": "5.5 Python Interface",
    "text": "5.5 Python Interface\nAs previously mentioned, python wrappers are included for most functions.\n\n5.5.1 Requirements For All Python Functions\nTo use these python functions, users need to have Python version 3.10 installed on their system.\nIf the user has a later version of Python installed (such as 3.11), and wants to keep that version of Python in their system, they will have to manage multiple Python versions on their system. One easy way to manage multiple environments (in our case, multiple versions of Python for users who need it) is to use Conda, which is a package and environment manager included in both Anaconda and Miniconda. For our purposes, Miniconda is more useful as a minimalistic way to install and use Conda. See this guide for information on how to install Miniconda – keep in mind that, at this time, this package requires Python 3.10 (NOT the latest version of Python), so when the guide prompts you to choose a Miniconda installer, you can choose a link for Python 3.10 from this list which is also linked in the aforementioned guide.\nAfter installing miniconda, refer to this section and later sections of the Conda user guide to set up an environment with Python 3.10.\nPython 3.10 conveniently comes with pip, the python package installer. It can be used to install various Python packages/modules that will be required to run the dsld Python functions. All of our Python functions  require rpy2, which allows for Python/R environment interaction.To install rpy2, run pip install rpy2 within the Python 3.10 environment, or see this link\n\n\n5.5.2 Running dsld Python Functions\nAs an example, let’s use the python interface for dsldTakeALookAround (the python-equivalent function is called dsldPyTakeALookAround)\nto view tabular information on how data features relate to Y and S. The python function allows users to enter arguments using Python data types and converts the resultant R dataframe into a Pandas dataframe.\nDifferent python functions will require different libraries depending on what the function does – Pandas  is required for dsldPyTakeALookAround.These libraries can also (usually) be installed with pip\n\nThe user should open the Python Shell Prompt by typing python from the package Python directory (/dsld/inst/Python) in the terminal/cmd prompt. Then, to run the Python function, we can run the following commands in the python shell:\n\nfrom dsldTakeALook_Py_R import dsldPyTakeALookAround    \nimport rpy2.robjects as robjects   \nrobjects.r['data']('svcensus')    \ndata = robjects.r('svcensus')    \nresult = dsldPyTakeALookAround(data, 'wageinc', 'gender')   \nprint(result)\n\nThe result should be the same information from the R dataframe, but converted to a Pandas dataframe."
  }
]