<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science Looks at Discrimination - 3&nbsp; Part II: Discovering/Mitigating Bias in Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Appendices.html" rel="next">
<link href="./PartI.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./PartII.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Part II:  Discovering/Mitigating Bias  in Machine Learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science Looks at Discrimination</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Motivating Examples</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PartI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Part I: Adjustment for Confounders</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PartII.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Part II: Discovering/Mitigating Bias in Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Appendices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Author Bios</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals"><span class="header-section-number">3.1</span> Goals</a></li>
  <li><a href="#comparison-to-part-i" id="toc-comparison-to-part-i" class="nav-link" data-scroll-target="#comparison-to-part-i"><span class="header-section-number">3.2</span> Comparison to Part I</a>
  <ul class="collapse">
  <li><a href="#deciding-proxies" id="toc-deciding-proxies" class="nav-link" data-scroll-target="#deciding-proxies"><span class="header-section-number">3.2.1</span> Deciding proxies</a></li>
  </ul></li>
  <li><a href="#measuring-utility" id="toc-measuring-utility" class="nav-link" data-scroll-target="#measuring-utility"><span class="header-section-number">3.3</span> Measuring utility</a></li>
  <li><a href="#measuring-unfairness" id="toc-measuring-unfairness" class="nav-link" data-scroll-target="#measuring-unfairness"><span class="header-section-number">3.4</span> Measuring unfairness</a>
  <ul class="collapse">
  <li><a href="#s-correlation" id="toc-s-correlation" class="nav-link" data-scroll-target="#s-correlation"><span class="header-section-number">3.4.1</span> S-Correlation</a></li>
  <li><a href="#demographic-parity" id="toc-demographic-parity" class="nav-link" data-scroll-target="#demographic-parity"><span class="header-section-number">3.4.2</span> Demographic Parity</a></li>
  <li><a href="#equalized-odds" id="toc-equalized-odds" class="nav-link" data-scroll-target="#equalized-odds"><span class="header-section-number">3.4.3</span> Equalized Odds</a></li>
  <li><a href="#the-fairness-package" id="toc-the-fairness-package" class="nav-link" data-scroll-target="#the-fairness-package"><span class="header-section-number">3.4.4</span> The <strong>fairness</strong> package</a></li>
  </ul></li>
  <li><a href="#remedies" id="toc-remedies" class="nav-link" data-scroll-target="#remedies"><span class="header-section-number">3.5</span> Remedies</a></li>
  <li><a href="#dsldqefairrf" id="toc-dsldqefairrf" class="nav-link" data-scroll-target="#dsldqefairrf"><span class="header-section-number">3.6</span> <strong>dsldQeFairRF</strong></a></li>
  <li><a href="#dsldqefairknn" id="toc-dsldqefairknn" class="nav-link" data-scroll-target="#dsldqefairknn"><span class="header-section-number">3.7</span> <strong>dsldQeFairKNN</strong></a></li>
  <li><a href="#remedies-based-on-shrunken-linear-models" id="toc-remedies-based-on-shrunken-linear-models" class="nav-link" data-scroll-target="#remedies-based-on-shrunken-linear-models"><span class="header-section-number">3.8</span> Remedies based on “shrunken” linear models</a>
  <ul class="collapse">
  <li><a href="#the-dsldfgrrm-function" id="toc-the-dsldfgrrm-function" class="nav-link" data-scroll-target="#the-dsldfgrrm-function"><span class="header-section-number">3.8.1</span> The <strong>dsldFgrrm</strong> function</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Part II: Discovering/Mitigating Bias in Machine Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div style="page-break-after: always;"></div>
<p>In modern applications of machine learning as a predictive modeling tool, it would be irresponsible to produce a model that biases one sensitive group over another. As such, it becomes imperative to find methods of uncovering and reducing any such biases.</p>
<section id="goals" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="goals"><span class="header-section-number">3.1</span> Goals</h2>
<p>There are two main aspects of fair machine learning:</p>
<ul>
<li><p><strong>Measuring unfairness</strong>: A number of measures have been proposed.</p></li>
<li><p><strong>Reducing unfairness</strong>: For a given ML algorithm, how can we ameliorate its unfairness, yet still maintain an acceptable utility (predictive power) level?</p></li>
</ul>
<p>In our earlier COMPAS example: Is the risk assessment tool biased against African-Americans? And if so, how can we reduce that bias while still maintaining good predictive ability?</p>
</section>
<section id="comparison-to-part-i" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="comparison-to-part-i"><span class="header-section-number">3.2</span> Comparison to Part I</h2>
<div class="page-columns page-full"><p>First, recall our notation: </p><div class="no-row-height column-margin column-container"><span class="">As before with C, think of X as, at first, consisting of all variables other than Y and S, but then selecting some of X as O, with X then being all variables but Y, S and O.</span></div></div>
<ul>
<li><p>Y: outcome variable, to be predicted</p></li>
<li><p>S: sensitive variable</p></li>
<li><p>O: proxy variables</p></li>
<li><p>X: other variables to be used to predict Y</p></li>
</ul>
<p>We wish to predict Y from X and O, omitting S, but with concern that we may be indirectly using S via O. Contrast this from our material in Part I:</p>
<ul>
<li><p>In Part I, we fit models for predicting Y, but with the goal of using such models to assess the effect of S on Y; we were not interested in actually predicting Y. We included S in our models, but wished to find variables C that were correlated with both Y and S, so as to avoid distorting our look at the impact of S on Y. Any X variable unrelated to Y was not of interest.</p></li>
<li><p>Here in Part II, prediction of Y is our central goal, rather than effect assessment. We will omit S, relying our prediction fully on X and partly on O. The variables O are related to S; the stronger the relation of an O variable to S, the less weight we will put on that variable in predicting Y.</p></li>
</ul>
<p>We will describe some common measures of unfairness shortly. But first, how do we choose the O variables?</p>
<section id="deciding-proxies" class="level3 page-columns page-full" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="deciding-proxies"><span class="header-section-number">3.2.1</span> Deciding proxies</h3>
<div class="page-columns page-full"><p>As with choosing confounders in Part I, the analyst may simply choose  all possible candidate O variables.<br>
the proxies based on his/her domain expertise. But a more formal approach may involve correlation. The function <strong>dsldOHunting</strong> calculates the correlations between S and</p><div class="no-row-height column-margin column-container"><span class="">It should be kept in mind that, as with any statistic, all utility and fairness measures are subject to sampling variation.</span></div></div>
<p>Here’s an example, using the COMPAS data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dsld)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: fairml</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: regtools</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: FNN</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>




*********************



Latest version of regtools at GitHub.com/matloff


Type ?regtools to see function list by category</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: qeML</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: rmarkdown</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: tufte</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>




*********************



  Navigating qeML:

      Type vignette("Quick_Start") for a quick overview!

      Type vignette("Function_List") for a categorized function list

      Type vignette("ML_Overview") for an introduction to machine learning</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'qeML'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked _by_ '.GlobalEnv':

    evalr</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Registered S3 method overwritten by 'GGally':
  method from   
  +.gg   ggplot2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(compas1) </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>cmp <span class="ot">&lt;-</span> compas1[,<span class="sc">-</span><span class="dv">3</span>]  <span class="co"># omit decile; we are developing our own risk tool</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dsldOHunting</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                               age juv_fel_count juv_misd_count juv_other_count
race.African-American -0.156957517    0.10966300    0.114479498      0.07478931
race.Asian             0.016385005   -0.01239908   -0.005952701     -0.01014768
race.Caucasian         0.147628857   -0.08028929   -0.090027168     -0.04101560
race.Hispanic          0.022247056   -0.03052358   -0.030760605     -0.02561435
race.Native American   0.002329938    0.03159615   -0.011713229      0.01229258
race.Other             0.002437568   -0.03832635   -0.020729116     -0.04670631
                      priors_count   sex.Female     sex.Male    c_jail_in
race.African-American   0.17509146 -0.041459873  0.041459873  0.002270144
race.Asian             -0.03073693 -0.021694502  0.021694502 -0.004670234
race.Caucasian         -0.10017594  0.068682539 -0.068682539  0.002112483
race.Hispanic          -0.06814588 -0.026131928  0.026131928 -0.006687662
race.Native American    0.02388670 -0.006505234  0.006505234 -0.004382357
race.Other             -0.08725931 -0.012952574  0.012952574  0.001166723
                        c_jail_out c_offense_date screening_date   in_custody
race.African-American  0.012784849  -0.0084427004   -0.004593993  0.060971397
race.Asian            -0.004725654  -0.0006422783   -0.004131066 -0.021151109
race.Caucasian        -0.005997440   0.0117472861    0.009717379 -0.027889553
race.Hispanic         -0.009454646  -0.0080839293   -0.004909945 -0.033209571
race.Native American  -0.004711864  -0.0083578408   -0.003890418 -0.005750257
race.Other            -0.001456317   0.0058788162   -0.002179469 -0.027193953
                       out_custody
race.African-American  0.071601855
race.Asian            -0.023006228
race.Caucasian        -0.036894524
race.Hispanic         -0.036660474
race.Native American  -0.003430048
race.Other            -0.027359200</code></pre>
</div>
</div>
<p>The output here suggests possibly using, say, the <strong>age</strong> and <strong>priors_count</strong> variables as proxies.</p>
<div class="page-columns page-full"><p>The function does not use the classic <em>Pearson product moment</em> correlation here, opting instead for <em>Kendall’s tau</em> correlation. Both are widely-used, and both take on values in [-1,], but while Pearson is geared toward continuous numeric variables, Kendall is also usable for  binary or ordinal integer-valued variables.</p><div class="no-row-height column-margin column-container"><span class="">Tau is defined in terms of <em>concordances</em> and <em>discordances</em>. Say we look at height and weight, and compare two people. If one of the people is both taller and heavier than the other, that is a concordance. If on the other hand, one is shorter but heavier than the other, that is a discordance. We consider all possible pairs of people in our dataset, then sets tau to the difference in concordance and discordance counts, divided by the number of pairs.</span></div></div>
</section>
</section>
<section id="measuring-utility" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="measuring-utility"><span class="header-section-number">3.3</span> Measuring utility</h2>
<p>Utility in ML classification algorithms in general, and both utility and fairness in the fair ML classification realm, often (but far from always) make use of quantitaties like False Positive Rate (FPR). So, let’s start by defining these rates.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The famous rates FPR etc.
</div>
</div>
<div class="callout-body-container callout-body">
<p>These are conditional probabilities, but it’s important not to confuse the event with the condition.</p>
<p>Consider binary Y classification problems, where we label Y is either <em>positive</em> (e.g.&nbsp;patient has the disease) or <em>negative</em> (e.g.&nbsp;patient does not have the disease).</p>
<p>After we fit our ML tool to predict Y, we use it for prediction. Consider a long period of time in which we do such predictions. During that time, define the following counts:</p>
<ul>
<li><p>FP: Count of the number of times we predict Y to be positive and actually it’s negative.</p></li>
<li><p>FN: Count of the number of times we predict Y to be negative and actually it’s positive.</p></li>
<li><p>TP: Count of the number of times we predict Y to be positive and actually it’s positive.</p></li>
<li><p>TN: Count of the number of times we predict Y to be negative and actually it’s negative.</p></li>
</ul>
<p>Then some key rates are: </p>
<ul>
<li><p>FPR: FP / (FP + TN) = P(guess Y positive | Y actually is negative)</p></li>
<li><p>TPR: TP / (TP + FN) = P(guess Y positive | Y actually is positive)</p></li>
<li><p>FNR: FN / (TP + FN) = P(guess Y negative | Y actually is positive)</p></li>
<li><p>TNR: TN / (FP + TN) = P(guess Y negative | Y actuually is negative)</p></li>
</ul>
<p>So for instance, FPR is the proportion of time we guess positive, <em>among those times</em> in which Y is actually negative. Two other common terms:</p>
<ul>
<li><p><em>recall</em>: same as TPR</p></li>
<li><p><em>sensitivity</em>: same as TPR</p></li>
<li><p><em>precision</em>: P(Y is actually positive | we guess Y is positive)= (TP + FN) / (TP + FP)</p></li>
</ul>
</div>
</div>
<div class="no-row-height column-margin column-container"><span class="callout-margin-content">It is standard to guess Y = 1 if the probability of that event is at least 0.5. But other thresholds can be used, with TPR and FPR varying as we vary the threshold. The <em>ROC curve</em> is the resulting graph of TPR vs.&nbsp;FPR; see <strong>qeROC</strong> in the <strong>qeML</strong> package.</span></div><div class="page-columns page-full"><p>A simple but quite common measure of utility in binary classification problems is the overall misclassification probability  of misclassification; <strong>qeML</strong> predictive functions report this in the <strong>testAcc</strong> component of the functions’ return object. There are many, many other measures.</p><div class="no-row-height column-margin column-container"><span class="">The <em>F1-score</em> has recently been especially popular in the ML research world. It is defined as the harmonic mean of precision and recall, and is thought to be especially useful in applications in which one class or the other is very rare.</span></div></div>
<p>For whatever reason, ML research has tended to focus on that binary Y case, and there are no analogous acronyms for numeric Y. Accuracy of the latter is handled simply as Mean Squared Prediction Error (MSPE), the average squared difference between predicted and actual Y value, or Mean Absolute Prediction Error (MAPE).</p>
</section>
<section id="measuring-unfairness" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="measuring-unfairness"><span class="header-section-number">3.4</span> Measuring unfairness</h2>
<div class="page-columns page-full"><p>Many unfairness criteria have been proposed. We preseent a few of them in this section.  It should be kept in mind that, just as there is no single ML algorithm that predicts the best in all applications, one’s choice of fairness measure also will depend on the given application.</p><div class="no-row-height column-margin column-container"><span class="">See the <a href="https://arxiv.org/pdf/1912.00761.pdf">Xiang and Raji</a> for an excellent analysis of the legal implications of various fairness measures.</span></div></div>
<section id="s-correlation" class="level3 page-columns page-full" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="s-correlation"><span class="header-section-number">3.4.1</span> S-Correlation</h3>
<p>A direct way to measure where Y and S are still related in spite of physically omitting the latter is to compute the correlation between predicted Y, to be denoted <span class="math inline">\(\hat{Y}\)</span> and S. As noted earlier, we use Kendall’s Tau correlation here.</p>
<div class="page-columns page-full"><p> For instance, let’s consider our mortgage example, say with k-Nearest Neighbors as our ML prediction tool:</p><div class="no-row-height column-margin column-container"><span class="">Here and below, to keep things simple, we will not use a holdout set.</span></div></div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dsld)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qeML)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>,<span class="at">yesYVal=</span><span class="st">'Yes'</span>) </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># look at the fitted model's probability of recidivism,</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># i.e. regression estimates</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> z<span class="sc">$</span>regests</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># the variable 'race' is an R factor, need a numeric</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>black <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(cmp<span class="sc">$</span>race<span class="sc">==</span><span class="st">'African-American'</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(black,probs,<span class="at">method=</span><span class="st">'kendall'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2343988</code></pre>
</div>
</div>
<p>That’s a pretty substantial correlation, definitely a cause for concern that our ML analysis here is unfair. Of course, it’s not the algorithm itself’s fault, but we must find a way to mitigate the problem.</p>
<div class="page-columns page-full"><p> An advantage of the S-Correlation measure is that it can also be used in non-classification problems, say predicting wage income, and take age as our sensitive variable S:</p><div class="no-row-height column-margin column-container"><span class="">Prediction of income might be of interest in, say, a marketing context. Actually, the field of marketing has been the subject of much concern in fair ML; e.g.&nbsp;see <a href="https://thefwa.com/cases/a-marketers-guide-to-machine-learning-fairness">FWA</a>.</span></div></div>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(svcensus)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(svcensus,<span class="st">'wageinc'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(z<span class="sc">$</span>regests,svcensus<span class="sc">$</span>age,<span class="at">method=</span><span class="st">'kendall'</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.225566</code></pre>
</div>
</div>
<p>So, again, our ML tool seems to be biased, this time in terms of age.</p>
</section>
<section id="demographic-parity" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="demographic-parity"><span class="header-section-number">3.4.2</span> Demographic Parity</h3>
<p>The criterion for demographic parity is that the same proportion of each sub-group within the sensitive feature is classified at equal rates for each of the possible outcomes</p>
<p>For example, let’s consider the COMPAS dataset again. Demographic Parity would require</p>
<p>P(predict recidivate | Black) = P(predict recidivate | white),</p>
<p>i.e.&nbsp;that the quanitty</p>
<p>(TP+FP) / (TP+FP+TN+FN)</p>
<p>has the same value for each race.</p>
<p>Below is an example using the COMPAS data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>,<span class="at">yesYVal=</span><span class="st">'Yes'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># determine which rows were for Black applicants, which not</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>BlackRows <span class="ot">&lt;-</span> <span class="fu">which</span>(cmp<span class="sc">$</span>race <span class="sc">==</span> <span class="st">'African-American'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>NonblackRows <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(cmp),BlackRows)  <span class="co"># all the others</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># regests, the output of qeKNN, is the vector of fitted probabilities</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (recall that for the Y = 0,1 case, the mean reduces to the probability</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># of a 1)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>BlackProbs <span class="ot">&lt;-</span> z<span class="sc">$</span>regests[BlackRows]</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>NonblackProbs <span class="ot">&lt;-</span> z<span class="sc">$</span>regests[NonblackRows]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># if a probability is &gt; 0.5, we will guess Y = 1, otherwise guess Y = 0;</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># conveniently, that's same as rounding to the nearest integer</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>BlackYhats <span class="ot">&lt;-</span> <span class="fu">round</span>(BlackProbs)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>NonblackYhats <span class="ot">&lt;-</span> <span class="fu">round</span>(NonblackProbs)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># again, recall that the mean of a bunch of 0s and 1s is the proportion</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># of 1s, i.e. the probability of a 1</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(BlackYhats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5073104</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(NonblackYhats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2724777</code></pre>
</div>
</div>
<p>That’s quite a difference! Overall, our ML model predicts about 51% of Black defendants to recidivate,j versus than 27% for non-Blacks.</p>
<p>However, such a criterion is generally considered too coarse, since it doesn’t account for possible differences in qualifications between the two groups. In other words, one must take confounders into account, as we did in Part I.</p>
</section>
<section id="equalized-odds" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="equalized-odds"><span class="header-section-number">3.4.3</span> Equalized Odds</h3>
<p>This criterion takes a retrospective view, asking in the case of COMPAS:</p>
<blockquote class="blockquote">
<p>Among those who recidivate, what proportion of them had been predicted to do so? And, does that proportion vary by race?</p>
</blockquote>
<p>If the answer to that second question is No, we say our prediction tool satisfies the Equalized Odds criterion.</p>
<p>So, Equalized Odds requires the quantity</p>
<p>TP / (TP+FN)</p>
<p>to be the same for each sensitive group.</p>
</section>
<section id="the-fairness-package" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="the-fairness-package"><span class="header-section-number">3.4.4</span> The <strong>fairness</strong> package</h3>
<p>This package calculates and graphically displays a wide variety of fairness criteria. For instance, let’s use it to evaluate the Equalized Odds criterion in the above COMPAS example.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fairness)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'fairness'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:fairml':

    compas</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">equal_odds</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>,<span class="at">probs=</span>z<span class="sc">$</span>regests)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$Metric
               African-American Asian    Caucasian    Hispanic Native American
Sensitivity           0.7536694     0    0.5943627   0.4810811       0.5714286
Equalized odds        1.0000000     0    0.7886253   0.6383184       0.7581952
Group size         2941.0000000    28 2055.0000000 501.0000000      14.0000000
                     Other
Sensitivity      0.5344828
Equalized odds   0.7091740
Group size     316.0000000

$Metric_plot</code></pre>
</div>
<div class="cell-output-display">
<p><img src="PartII_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
$Probability_plot</code></pre>
</div>
<div class="cell-output-display">
<p><img src="PartII_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Taking African-Americans as the base, we see that the Equalized Odds criterion was not met, even approximately. Nor was Demographic Parity.</p>
</section>
</section>
<section id="remedies" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="remedies"><span class="header-section-number">3.5</span> Remedies</h2>
<p>Having established that ML prediction models can be biased against certain sensitive groups, what remedies are available? The <strong>dsld</strong> package includes a few of these, presented in this section. Of course, all of them recognize a basic principle:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Fairness-Utility Tradefoff
</div>
</div>
<div class="callout-body-container callout-body">
<p>Of course, nothing comes for free: the inherent tradeoff of increasing fairness is reduced utility (reduced predictive power over the dataset). Thus, a means of balancing this tradeoff between fairness and utility becomes essential in any future implementations of machine learning.</p>
</div>
</div>
<p>Any algorithm for ameliorating unfairness will thus include one or more parameters that one can use to achieve a desired level of compromise between fairness and utility. The parameters essentially allow us to “dial” the weight that our proxies will play in predicting Y; lighter weight means more fairness but poorer utility, and vice versa.</p>
<p>Our context will be:</p>
<ul>
<li><p>Due to legal requirements or simply a desire for fairness, we will omit S from all analyses, other than for fairness assessment of our derived prediction tool.</p></li>
<li><p>But we are concerned about the impact of proxies, and have chosen a set of variables O to play this role.</p></li>
<li><p>We have chosen fairness and utility measures with which we will choose a desired point in the Fairness-Utility Tradefoff.</p></li>
</ul>
<p>Many, many fair ML algorithms have been proposed, most of which are technically complex. The ones we present here have been chosen (a) for their technical simplicity and (b) availability as R packages. We will begin with the simplest algorithms (which have been developed by one of the authors of this book).</p>
<p>Let’s take as a running example the COMPAS data, predicting recidivism, with age and prior convictions count as proxies. To illustrate prediction, we will predict the first case in the dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>newx <span class="ot">&lt;-</span> cmp[<span class="dv">1</span>,<span class="sc">-</span>(<span class="dv">7</span><span class="sc">:</span><span class="dv">8</span>)]  <span class="co"># just X and O, not Y and S</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dsldqefairrf" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="dsldqefairrf"><span class="header-section-number">3.6</span> <strong>dsldQeFairRF</strong></h2>
<p>This function fits an RF model, but with deweighting of the proxies.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">dsldQeFairRF</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>,</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">list</span>(<span class="at">age=</span><span class="fl">0.2</span>,<span class="at">priors_count=</span><span class="fl">0.1</span>),<span class="at">yesYVal=</span><span class="st">'Yes'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Warning: Split select weights used. Variable importance measures are only comparable for variables with equal weights.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>z<span class="sc">$</span>corrs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>African-American            Asian        Caucasian         Hispanic 
      0.21366424       0.15610522       0.16840232       0.20264208 
 Native American            Other 
      0.01015095       0.14011174 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(z,newx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$predClasses
[1] "No"

$probs
            No       Yes
[1,] 0.6555973 0.3444027</code></pre>
</div>
</div>
<p>Recall that in RFs, each tree will use a different random ordering of the X and O variables. At any given node in a tree, a variable is chosen at random to set up a possible split point. But here we are specifying that the age and priors count variables be used only 20% and 10% as often as other variables, in order to reduce their impact.</p>
<p>Yet the S-Correlation valuea are still substantial. We need to give the O variables even smaller weights or possibly include additional variables in our O set.</p>
</section>
<section id="dsldqefairknn" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="dsldqefairknn"><span class="header-section-number">3.7</span> <strong>dsldQeFairKNN</strong></h2>
<p>Remember, the common theme here is reducing the role played in prediction by the proxies O. How might this be done with k-NN?</p>
<p>An easy answer is to weight the distance metric. Ordinarily, if we move 3.2 meters to the right and then 1.1 meters forward, the distance between our new point and our original one is</p>
<p><span class="math inline">\(\sqrt{3.2^2+1.1^2} = 3.38\)</span></p>
<p>That puts equal weight in the left-right direction and the forward-back direction, which makes sense for geometric distance. But in data prediction, we can use different weights. In prediction wage income in the Census data, say, we can place large weight on age and occupation, and less weight on education. The above call would change to:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">dsldQeFairKNN</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>,</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">list</span>(<span class="at">age=</span><span class="fl">0.2</span>,<span class="at">priors_count=</span><span class="fl">0.1</span>),<span class="at">yesYVal=</span><span class="st">'Yes'</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>z<span class="sc">$</span>corrs</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(z,newx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="remedies-based-on-shrunken-linear-models" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="remedies-based-on-shrunken-linear-models"><span class="header-section-number">3.8</span> Remedies based on “shrunken” linear models</h2>
<p>One of the most striking advances in modern statistics was the discovery that classical estimators tend to be “too big,” and that one may improve accuracy by “shrinking” them. Here is the intuition:</p>
<p>Consider the example in <a href="PartI.html#sec-compas"><span>Section&nbsp;2.7</span></a>. The vector of estimated regression coefficients was</p>
<p>(12.46,-0.94,0.39,0.16,…)</p>
<p>We might (crudely) shrink this vector by multiplying by a factor of, say, 0.5, yielding</p>
<p>(6.23,-0.47,0.20,0.08,…)</p>
<p>The actual shrinkage mechanisms used in the regression context are much more complex than simply multiplying every component of the vector by the same constant, but this is the basic principle.</p>
<p>Why might this odd action be helpful? Shrinking introduces a bias, but reduces variance (roughly speaking, smaller variables vary less). If outliers (extreme values, not errors) are common in the data, these result in large estimator variance, possibly so much that shrinkage’s reduction in variance overwhelms our increase in bias.</p>
<p>Some readers may have heard of <em>ridge regression</em> and the <em>LASSO</em>, both of which impose shrinkage. For our fair ML context, though we wish to perform our shrinkage focusing on <em>only some</em> of the estimated regression coefficients, specifically those corresponding to our proxies. To be sure, it should be noted that non-proxy coefficients are affected too, but the main effects will be on the proxies.</p>
<p>Note that these estimators also have the potential ancillary benefit obtained from shrinkage in general, i.e.&nbsp;better utility.</p>
<section id="the-dsldfgrrm-function" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="the-dsldfgrrm-function"><span class="header-section-number">3.8.1</span> The <strong>dsldFgrrm</strong> function</h3>
<p>This is a wrapper to a corresponding function in the <strong>fairml</strong> package, based on <a href="https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf">a paper</a> by the authors of the package.</p>
<p>Let’s see what it does with the COMPAS data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dsldFgrrm</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>,<span class="at">unfairness=</span><span class="fl">0.1</span>,<span class="at">family=</span><span class="st">'binomial'</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dsldFgrrm</span>(cmp,<span class="st">'two_year_recid'</span>,<span class="st">'race'</span>,<span class="at">unfairness=</span><span class="fl">0.01</span>,<span class="at">family=</span><span class="st">'binomial'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <strong>unfairness</strong> argument is a number in (0,1]. The smaller the value, the fairer the fit. The first value, 0.1, gave an estimate coefficient for Caucasian of -0.73, while using 0.01 reduced this to -0.15. (Note that the base for the dummy variables is apparently African-American.] But both values are small relative some of the other coefficients.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./PartI.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Part I: Adjustment for Confounders</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Appendices.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Author Bios</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>